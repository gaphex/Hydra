{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import tweepy\n",
    "import random\n",
    "import urllib2\n",
    "import Stemmer\n",
    "import sqlite3\n",
    "import operator\n",
    "import pymorphy2\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import stdout\n",
    "from lshash import LSHash\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Hyperion():\n",
    "    def __init__(self, data):\n",
    "        print 'Invoking Hyperion...'\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        self.data = data\n",
    "        \n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def preprocess(self):\n",
    "        \n",
    "        print 'Preprocessing...this may take a while'\n",
    "        t0 = time.time()\n",
    "        terms = []\n",
    "        words = []\n",
    "\n",
    "        n = len(data.index)\n",
    "\n",
    "        for i in range(n):\n",
    "            terms.append(self.processContents(self.data.content_lower[i]))\n",
    "\n",
    "        self.data['terms'] = terms[:]\n",
    "\n",
    "        for i in range(n):\n",
    "            words += terms[i]\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "        interestingVocab = [x[0] for x in sortedDist]\n",
    "\n",
    "        #Find TF-IDF\n",
    "\n",
    "        trainingList = []\n",
    "        for i in range(n):\n",
    "            trainingList.append(' '.join(data['terms'][i]))\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "        self.tfidf = self.vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization\n",
    "\n",
    "        print 'Building vocab and TF-IDF matrix took', time.time()-t0, 'seconds'\n",
    "        print 'Vocab Length =', len(interestingVocab), '    Vector dimensionality =', n\n",
    "        return(self.vectorizer, self.tfidf)\n",
    "\n",
    "\n",
    "    def progress(self, i, n):\n",
    "        stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "        stdout.flush()\n",
    "        if i == n-1:\n",
    "            stdout.write(\"\\r100%\")\n",
    "            print(\"\\r\\n\")\n",
    "        \n",
    "    def median(self,lst):\n",
    "        return np.median(np.array(lst))\n",
    "\n",
    "    def extract_urls(self, lst):\n",
    "        urls = []\n",
    "        for i in lst:\n",
    "            for j in i.split(' '):\n",
    "                if j.startswith('http'):\n",
    "                    urls.append(j)\n",
    "        return urls\n",
    "\n",
    "    def resolve_url(self, starturl):\n",
    "        try:\n",
    "            req = urllib2.Request(starturl)\n",
    "            res = urllib2.urlopen(req, timeout = 2)\n",
    "            finalurl = res.geturl()\n",
    "            return finalurl\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def setup(self, vectorizer, tf_idf):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tfidf = tf_idf\n",
    "        print 'Setup complete '\n",
    "        \n",
    "\n",
    "    def findSimilarTweets(self, queryTweet, threshold, maxNumber = 0, log = False):\n",
    "\n",
    "        t0 = time.time()\n",
    "        \n",
    "        processedTweet = ' '.join(self.processContents(queryTweet))\n",
    "        queryTweetRepresentation = self.vectorizer.transform([processedTweet])\n",
    "\n",
    "        cosine_similarities = cosine_similarity(queryTweetRepresentation, self.tfidf)[0]\n",
    "        totalMatchingTweets = len(cosine_similarities[cosine_similarities>threshold])\n",
    "\n",
    "        if maxNumber:\n",
    "            totalMatchingTweets = min(totalMatchingTweets, maxNumber)\n",
    "        indices = cosine_similarities.argsort()[::-1][:totalMatchingTweets]\n",
    "        elapsed_time = time.time() - t0\n",
    "        if len(indices) > 25:\n",
    "            print 'Query:', queryTweet\n",
    "        return indices\n",
    "\n",
    "    def findOut(self, cluster):\n",
    "        lst = list(self.data['content'][cluster])\n",
    "\n",
    "        redirects = []\n",
    "        keywords = []\n",
    "\n",
    "        urls = self.extract_urls(lst)\n",
    "        for url in urls:\n",
    "            redirects.append(self.resolve_url(url))\n",
    "\n",
    "        corpus = ' '.join(lst).lower()\n",
    "        corpus = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', corpus)\n",
    "        words = []\n",
    "        tokenized = nltk.tokenize.word_tokenize(corpus)\n",
    "        for word in tokenized:\n",
    "            words.append(word)\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        moreThan3 = [x for x in sortedDist if len(x[0]) > 3 and x[1] > 3]\n",
    "        for i in moreThan3:\n",
    "            keywords.append(i[0])\n",
    "\n",
    "        return(keywords, redirects)\n",
    "\n",
    "    def resolve(self, masterCluster):\n",
    "        t0 = time.time()\n",
    "        k, r = self.findOut(masterCluster)\n",
    "\n",
    "        insta = []\n",
    "        swarm = []\n",
    "\n",
    "        print 'Found', len(r), 'anonymous links, investigating...'\n",
    "        for i, u in enumerate(r):\n",
    "            if u:\n",
    "                if 'instagram' in u:\n",
    "                    insta.append(u)\n",
    "                if 'swarmapp' in u:\n",
    "                    swarm.append(u)\n",
    "\n",
    "        print 'Resolving took', time.time()-t0\n",
    "        return ((k, insta, swarm))\n",
    "\n",
    "    def overlap(self, r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b):\n",
    "        #Overlapping rectangles overlap both horizontally & vertically\n",
    "        return self.range_overlap(r1l, r1r, r2l, r2r) and self.range_overlap(r1b, r1t, r2b, r2t)\n",
    "\n",
    "    def range_overlap(self, a_min, a_max, b_min, b_max):\n",
    "        #Neither range is completely greater than the other\n",
    "        return (a_min <= b_max) and (b_min <= a_max)\n",
    "\n",
    "    def assess(self, clusters):\n",
    "        data = []\n",
    "        for c in clusters:\n",
    "\n",
    "            xs = [t[0] for t in c]\n",
    "            ys = [t[1] for t in c]\n",
    "            z = zip(xs, ys)\n",
    "            n = len(z)\n",
    "\n",
    "            dx = max(xs) - min(xs)\n",
    "            dy = max(ys) - min(ys)\n",
    "\n",
    "            mx = self.median(xs)\n",
    "            my = self.median(ys)\n",
    "\n",
    "            s=(dx+0.001)*(dy+0.001)\n",
    "            score = n/s\n",
    "\n",
    "            data.append((z,n,score,dx,dy,mx,my))\n",
    "            data = filter(lambda a: a[1] > 5, data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def findUnique(self, stats):\n",
    "\n",
    "        for v in range(3):\n",
    "            for i in stats:\n",
    "                r1l = i[5]-2*i[3]\n",
    "                r1r = i[5]+2*i[3]\n",
    "                r1b = i[6]-2*i[4]\n",
    "                r1t = i[6]+2*i[4]\n",
    "                for j in stats:\n",
    "                    r2l = j[5]-2*j[3]\n",
    "                    r2r = j[5]+2*j[3]\n",
    "                    r2b = j[6]-2*j[4]\n",
    "                    r2t = j[6]+2*j[4]\n",
    "                    if self.overlap(r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b) and i!=j:\n",
    "                        if i[1] >= j[1]:\n",
    "                            try:\n",
    "                                stats.remove(j)\n",
    "                            except:\n",
    "                                pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                stats.remove(i)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    def performClusterisation(self, indices, thSp, thTm):\n",
    "    \n",
    "        #Spatial Clustering\n",
    "        y = list(data['lat'][indices])\n",
    "        x = list(data['long'][indices])\n",
    "        tid = indices\n",
    "\n",
    "        z = filter(lambda a: a[0] != (55.753301, 37.619899), zip(zip(x,y), tid))\n",
    "        x = []\n",
    "        y = []\n",
    "        tid = []\n",
    "\n",
    "        for t in z:\n",
    "            x.append(t[0][0])\n",
    "            y.append(t[0][1])\n",
    "            tid.append(t[1])\n",
    "\n",
    "        crds = zip(x,y)\n",
    "        spUniques = self.findCluster(crds, tid, 34, thSp, thTm, 'space')\n",
    "        if len(spUniques):\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', len(spUniques), 'spatial clusters with a total of', sum([len(t[0]) for t in spUniques]), 'points among', len(indices)\n",
    "\n",
    "        for w in spUniques:\n",
    "            print len(w[0]), 'points at', w[3], w[4]\n",
    "\n",
    "\n",
    "        #Temporal Clustering\n",
    "        tmvc = []\n",
    "        for t in data['created_at'][indices]:\n",
    "            l = t.split(' ')\n",
    "            date = l[0].split('-')\n",
    "            time = l[1].split(':')\n",
    "            datehash = int(date[1]) * 30 + int(date[2])\n",
    "            timehash = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
    "            tmvc.append((datehash*400, timehash))\n",
    "\n",
    "        tpUniques = self.findCluster(tmvc, tid, 34, thSp, thTm, 'time')\n",
    "        dm = len(tpUniques)\n",
    "\n",
    "        if dm:\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', dm, 'temporal clusters with a total of', sum([len(t[0]) for t in tpUniques]), 'points among', len(indices)\n",
    "        \n",
    "\n",
    "        for w in tpUniques:\n",
    "            date = w[3]/400\n",
    "            time = w[4]\n",
    "            print len(w[0]), 'points at', int(date//30),'.',int(date%30),'--',int(time//3600),\n",
    "            print ':',int(time%3600//60),':',int(time%3600%60)\n",
    "\n",
    "        print ''\n",
    "        return ((spUniques, tpUniques))\n",
    "\n",
    "    def findCluster(self, vecs, tids, cln, thSp, thTm, mode):\n",
    "\n",
    "        maxn = 0\n",
    "        maxc = []\n",
    "        dim = len(vecs[0])\n",
    "        data = zip(vecs, tids)\n",
    "\n",
    "\n",
    "        if mode == 'space':\n",
    "            metric = \"euclidean\"\n",
    "            th = thSp\n",
    "            rat = 1\n",
    "        if mode == 'time':\n",
    "            metric = 'hamming'\n",
    "            th = thTm\n",
    "            rat = 1\n",
    "\n",
    "        lsh = LSHash(cln, dim)\n",
    "        for i in vecs:\n",
    "            lsh.index(i)\n",
    "\n",
    "        pivots = list(set(random.sample(vecs, int(rat*len(vecs)))))\n",
    "\n",
    "        for i in pivots:\n",
    "            mxc = []\n",
    "            dists = []\n",
    "            u = lsh.query(i, distance_func=metric)\n",
    "\n",
    "            for j in u:\n",
    "                dists.append(j[1])\n",
    "                mxc.append(j[0])\n",
    "\n",
    "            r = self.cover(dists, th)\n",
    "            maxc.append(mxc[0:r])\n",
    "\n",
    "\n",
    "        scr = self.assess(maxc)\n",
    "        rez = self.findUnique(scr)\n",
    "\n",
    "        clusters = []\n",
    "        for u in rez:\n",
    "            cIDs = []\n",
    "            for t in u[0]:\n",
    "                for d in data:\n",
    "                    if t == d[0]:\n",
    "                        cIDs.append(d[1])\n",
    "            clusters.append((cIDs, u[3], u[4], u[5], u[6]))\n",
    "\n",
    "        clusters = filter(lambda a: len(a[0]) > 6, clusters)\n",
    "\n",
    "        return (clusters)\n",
    "\n",
    "    def cover(self, dat, th):\n",
    "        sum = 0\n",
    "        t = 0\n",
    "        while sum < th*len(dat) and t < 0.5*len(dat):\n",
    "            sum += dat[t]\n",
    "            t += 1\n",
    "        return t\n",
    "\n",
    "    def doQuery(self, query, th_NN, th_SP, th_TM):\n",
    "        t0 = time.time()\n",
    "        indices = self.findSimilarTweets(query, th_NN)\n",
    "        print len(indices), 'points passed preprocessing'\n",
    "\n",
    "        if len(indices):\n",
    "            t = self.performClusterisation(indices, th_SP, th_TM)\n",
    "            print \"Query processing took\", time.time()-t0, 'seconds'\n",
    "\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Preprocessing...this may take a while\n",
      "Building vocab and TF-IDF matrix took 155.538730145 seconds\n",
      "Vocab Length = 414550     Vector dimensionality = 1315775\n"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect('tweetsSpring.db')\n",
    "data = pd.read_sql(\"SELECT * from tweets\",con) #WHERE content LIKE '%гроза%' COLLATE NOCAS\n",
    "\n",
    "hyperion = Hyperion(data)\n",
    "vec, tf = hyperion.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: я в кинотеатре\n",
      "355 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 39 points among 355\n",
      "19 points at 55.7553515 37.618101\n",
      "9 points at 55.782217 37.716641\n",
      "11 points at 55.8512005 37.4431215\n",
      "--------------------------------------------------------------\n",
      "Found 1 temporal clusters with a total of 7 points among 355\n",
      "7 points at 4 . 15 -- 16 : 48 : 22\n",
      "\n",
      "Query processing took 1.76247406006 seconds\n"
     ]
    }
   ],
   "source": [
    "u = hyperion.doQuery(u'я в кинотеатре', 0.34, 0.000005, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Setup complete \n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)\n",
    "hyperion.setup(vec, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "media = []\n",
    "for clusterType in u:\n",
    "    for cluster in clusterType:\n",
    "        z = hyperion.resolve(cluster[0])\n",
    "        print media\n",
    "        media.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for photo in media[0][1]:\n",
    "    print photo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "CONSUMER_KEY = ['KpfGPpsl5Dn03Lb5wzvQfEaMc',\n",
    "                '13AqFSrFdFv7rdLVOGvzJCkmp',\n",
    "                '45RuEYLg5eVTYyEGuyEerplyY',\n",
    "                '3qTMdAYKxctRe69HMDqyeNST2',\n",
    "                'JJkrFkKGlhDIEgj2eTrQ']\n",
    "\n",
    "CONSUMER_SECRET = ['UWRvjR3CHsducO1i7268F24C3M9UJu5U7p2u4kh6Ds6QMDdKCg',\n",
    "                   'LVaOSMsMBWl4FmgthjNPWMnkKe7MXKXrmu5uL6JnJWIhHieDxR',\n",
    "                   'LBnbBTIAhtYYBU6RWeyCzgIcJannob7bPrzg3dMqFuRDLJnbHp',\n",
    "                   'Jwwv3wHzL2jtYMHylakpjmDxf5SgvAwexFGfEoCFHw92f65lnK',\n",
    "                   'H7hmUQXqXseKbj1WnKFMnaURyQbBaDeyK3DAAwLI']\n",
    "\n",
    "OAUTH_TOKEN = ['2181757628-o8IOmHBelyhVM6KEkkT50ZLIbv4fj6llW6KSjpd',\n",
    "               '175663996-ZNL1MivJASYSxWsNXlxNHnQhmLHDegH9VdVfATsL',\n",
    "               '175663996-lQRf1JNjvR1fVILTtoEH4FHVQ1sLtPa0IIa8lMog',\n",
    "               '2841198550-rlPUcMyCj8rk3Yv6XxGJWk0ELCCUGUrxhvYyAa6',\n",
    "               '2181757628-0n3FpGEtoob0qum7IMeN3R0oV1kg5STZwmXNa9Q']\n",
    "\n",
    "OAUTH_TOKEN_SECRET = ['cyLlZtQyv4rgcWA5pGaXLtGJaFqD4PGOlxSdb4ECVzoSP',\n",
    "                      'gJOHvvlcObkiu7Qd91WapTFwnOVsisdoeMBUHxcFfzBac',\n",
    "                      'eQ0SUwziSUgRs72HJzWpU9IAlVP92X9YJsGHOPrWUctw3',\n",
    "                      '9b36g1wXLzn1yB0FGIoT9eACxPPpaZVfESnmRYcDYk3wv',\n",
    "                      'MqgrZHb8CMyNqFJn36YmCtLUQ5rqNUzX2IxWNfQdHQ6t7']\n",
    "\n",
    "auths = []\n",
    "\n",
    "up = 55.96\n",
    "down = 55.49\n",
    "right = 37.97\n",
    "left = 37.32\n",
    "\n",
    "for i in range(5):\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY[i], CONSUMER_SECRET[i])\n",
    "    auth.set_access_token(OAUTH_TOKEN[i], OAUTH_TOKEN_SECRET[i])\n",
    "    auths.append(auth)\n",
    "\n",
    "\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            tid = status.id_str\n",
    "            usr = status.author.screen_name.strip()\n",
    "            txt = status.text.strip()\n",
    "\n",
    "            indices = hyperion.findSimilarTweets(txt, 0.5)\n",
    "            \n",
    "            if len(indices) > 34:\n",
    "                try:\n",
    "                    hyperion.doQuery(txt, 0.34, 0.000005, 5000)\n",
    "                except:\n",
    "                    print 'Caught exception during clusterization'\n",
    "    \n",
    " \n",
    "        except Exception as e:\n",
    "            # Most errors we're going to see relate to the handling of UTF-8 messages (sorry)\n",
    "            print('BULLSHIT', e)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_disconnect(self, notice):\n",
    "        print notice\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: @morozovbest Порошенко непременно навестит переломанного Керри и подарит ему футбольный мяч\n",
      "Query: 🔝 http://t.co/F41nTHUv4N\n",
      "Query: какие они очаровательные 😻 http://t.co/ZsKcGtEgKG\n",
      "Query: какие они очаровательные 😻 http://t.co/ZsKcGtEgKG\n",
      "1178 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 7 spatial clusters with a total of 88 points among 1178\n",
      "21 points at 55.757911 37.609958\n",
      "11 points at 55.896612 37.398674\n",
      "7 points at 55.606148 37.597804\n",
      "8 points at 55.730543 37.816314\n",
      "9 points at 55.929383 37.844581\n",
      "14 points at 55.7370125 37.474013\n",
      "18 points at 55.8931545 37.6215015\n",
      "--------------------------------------------------------------\n",
      "Found 1 temporal clusters with a total of 7 points among 1178\n",
      "7 points at 4 . 14 -- 12 : 18 : 17\n",
      "\n",
      "Query processing took 7.7037229538 seconds\n",
      "Query: Собрались всем классом называется 😄\n",
      "Любимые 😚❤️ @ Столешников переулок https://t.co/ez6WnX4CB4\n",
      "Query: Это просто не возможно не прокомментировать!\n",
      "Query: Жизнь висит на нитке,а думают все о прибытке?!! #апокалипсис2016 #крахСША #мифология #US #наемники #stopTerrorUSA  https://t.co/TjIHcAnyDp\n",
      "Query: и я все еще не могу отойти от того вайна \n",
      "гспд\n",
      "Query: @NATO http://t.co/iuiGmeQFdL\n",
      "Query: Слишком много опасных моментов за сегодня\n",
      "Query: Слишком много опасных моментов за сегодня\n",
      "434 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 15 points among 434\n",
      "7 points at 55.622455 37.490585\n",
      "8 points at 55.7636135 37.615162\n",
      "\n",
      "Query processing took 1.79436182976 seconds\n",
      "Query: Через полчаса лето!!\n",
      "Query: Через полчаса лето!!\n",
      "389 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 15 points among 389\n",
      "7 points at 55.756574 37.656901\n",
      "8 points at 55.8063395 37.498773\n",
      "\n",
      "Query processing took 1.62681102753 seconds\n",
      "Query: @chiuuri сладких снов :з\n",
      "Query: @chiuuri сладких снов :з\n",
      "2061 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 5 spatial clusters with a total of 146 points among 2061\n",
      "61 points at 55.891143 37.7203465\n",
      "12 points at 55.980498 37.1561835\n",
      "19 points at 55.622485 37.490667\n",
      "22 points at 55.6795015 37.851008\n",
      "32 points at 55.817334 37.499944\n",
      "--------------------------------------------------------------\n",
      "Found 5 temporal clusters with a total of 55 points among 2061\n",
      "11 points at 4 . 1 -- 21 : 53 : 6\n",
      "9 points at 4 . 18 -- 23 : 37 : 53\n",
      "15 points at 4 . 23 -- 20 : 44 : 48\n",
      "12 points at 5 . 19 -- 20 : 3 : 29\n",
      "8 points at 5 . 23 -- 23 : 17 : 11\n",
      "\n",
      "Query processing took 34.7458641529 seconds\n",
      "Query: Государева Дума и метро) #moscow #russia #vscocam @ Государственная Дума Федерального Собрания… https://t.co/1NkvikLYJe\n",
      "Query: Государева Дума и метро) #moscow #russia #vscocam @ Государственная Дума Федерального Собрания… https://t.co/1NkvikLYJe\n",
      "151 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 118 points among 151\n",
      "118 points at 55.758048 37.616049\n",
      "\n",
      "Query processing took 0.914762973785 seconds\n",
      "Query: #Ирак. Данный вид флага был утвержден в 2008 и существует по настоящее время. С 1920г. Он менялся 6… https://t.co/Qg2cLsDmhm\n",
      "Query: @Series_Fun98 да, в группе по бритт сноу\n",
      "Query: Лежу на диване, смотрю как папа играет в FarCry и вообще все офигенно\n",
      "Query: http://t.co/gM4b9M2IfE\n",
      "Query: И мы перешли на новую ступень наших отношений. И свалились с нее, т.к. у флэчера в душе заиграла десятилетка\n",
      "Query: @viktessa и часами. Не.\n",
      "Query: @viktessa и часами. Не.\n",
      "2317 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 5 spatial clusters with a total of 128 points among 2317\n",
      "9 points at 55.994064 37.185103\n",
      "14 points at 55.90129 37.40154\n",
      "50 points at 55.599456 37.669286\n",
      "17 points at 55.680597 37.858167\n",
      "38 points at 55.7599025 37.620751\n",
      "--------------------------------------------------------------\n",
      "Found 5 temporal clusters with a total of 40 points among 2317\n",
      "7 points at 4 . 16 -- 9 : 20 : 18\n",
      "11 points at 4 . 24 -- 17 : 13 : 33\n",
      "7 points at 4 . 0 -- 20 : 32 : 35\n",
      "8 points at 5 . 19 -- 18 : 35 : 41\n",
      "7 points at 5 . 13 -- 14 : 5 : 9\n",
      "\n",
      "Query processing took 30.2573149204 seconds\n",
      "Query: @tema_vanilla ну ладно(\n",
      "Иногда действительно забавно((\n",
      "Query: @tema_vanilla ну ладно(\n",
      "Иногда действительно забавно((\n",
      "744 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 31 points among 744\n",
      "8 points at 55.8033385 37.498818\n",
      "7 points at 55.599455 37.669021\n",
      "16 points at 55.8911415 37.7203635\n",
      "\n",
      "Query processing took 3.13643598557 seconds\n",
      "Query: Спокойной ночи⭐️🌟✨💫\n",
      "Query: Спокойной ночи⭐️🌟✨💫\n",
      "2979 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 8 spatial clusters with a total of 205 points among 2979\n",
      "30 points at 55.681949 37.848151\n",
      "15 points at 55.3256455 37.8123545\n",
      "32 points at 55.664957 37.498989\n",
      "12 points at 55.501481 38.364402\n",
      "30 points at 55.8766715 37.688026\n",
      "20 points at 56.038302 37.245166\n",
      "25 points at 55.600836 37.653428\n",
      "41 points at 55.8063705 37.498802\n",
      "--------------------------------------------------------------\n",
      "Found 4 temporal clusters with a total of 56 points among 2979\n",
      "15 points at 4 . 23 -- 20 : 25 : 2\n",
      "17 points at 5 . 13 -- 21 : 4 : 33\n",
      "17 points at 4 . 1 -- 20 : 28 : 7\n",
      "7 points at 5 . 23 -- 23 : 46 : 23\n",
      "\n",
      "Query processing took 67.3033671379 seconds\n",
      "Query: #@Poroshenko,  #Poroshenko, эй потомок Вия на плече у тебя и не один, не давит, ножки не болят ,Петро!!!\n",
      "Query: Потанина тп\n",
      "Query: @mariamatildusha @lexusarh2009 Цаплиенко это редкий по сути гандон.Мразь\n",
      "Query: САМЫЕ СУКА АХУЕННЫЕ РЕБЯТА http://t.co/hT6Hp6vOP8\n",
      "Query: САМЫЕ СУКА АХУЕННЫЕ РЕБЯТА http://t.co/hT6Hp6vOP8\n",
      "531 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 4 spatial clusters with a total of 32 points among 531\n",
      "8 points at 55.7683165 37.643781\n",
      "8 points at 55.807954 37.4976705\n",
      "9 points at 55.737825 37.470683\n",
      "7 points at 55.642395 37.521987\n",
      "\n",
      "Query processing took 1.58270192146 seconds\n",
      "Query: @NATO http://t.co/HTICba1ieH\n",
      "Query: @ananaskalee плюс плюс\n",
      "Query: @ananaskalee плюс плюс\n",
      "518 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 23 points among 518\n",
      "7 points at 55.891147 37.720395\n",
      "8 points at 55.798446 37.4001105\n",
      "8 points at 55.763829 37.6352145\n",
      "\n",
      "Query processing took 1.89278292656 seconds\n",
      "Query: @anankafishbeyn9 Сук, ты прям как знаешь, где у меня плохо вайфай ловит азахазад\n",
      "Query: @quarter_miyo ага http://t.co/rwI0IZo8GL\n",
      "Query: @frankjoy_gw прям капельку совсем\n",
      "Query: ЕГЭ по истории Украины. http://t.co/kxXLQ009SL\n",
      "Query: ЕГЭ по истории Украины. http://t.co/kxXLQ009SL\n",
      "401 points passed preprocessing\n",
      "\n",
      "Query processing took 1.69189310074 seconds\n",
      "Query: @pashkal не, ну даже wi-fi почти работает. Щас наверное все уснут и совсем заработает.\n",
      "Query: Танцы, танцы танцует девчонка,\n",
      "А парни пусть постоят в сторонке.\n",
      "И в каждой ноте движение… https://t.co/BvmUDcAd2k\n",
      "Query: Танцы, танцы танцует девчонка,\n",
      "А парни пусть постоят в сторонке.\n",
      "И в каждой ноте движение… https://t.co/BvmUDcAd2k\n",
      "257 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 21 points among 257\n",
      "14 points at 55.756689 37.614108\n",
      "7 points at 55.807211 37.522708\n",
      "\n",
      "Query processing took 1.05461096764 seconds\n",
      "Query: моё имя в такой перекладке будет звучать примерно как Арьяд Сивелиастовре\n",
      "Query: Делала с утра немецкий\n",
      "\n",
      "@\n",
      "\n",
      "В девять вечера звонит Цепков и отменяет пары\n",
      "\n",
      "@\n",
      "\n",
      "Ну пасиба\n",
      "Query: @NATO http://t.co/JuUUmAq72J\n",
      "Query: congratulations @arminvanbuuren #arminvanbuuren read Twitter 100 !!! you're the #best #COOL #LIFEFISHKA http://t.co/yvsAoEsLjT\n",
      "Query: @kristin061996 в 00-00 орех в окно \"летоооо \" ахахахха\n",
      "Query: ой какая неловкость\n",
      "Query: ой какая неловкость\n",
      "1368 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 6 spatial clusters with a total of 76 points among 1368\n",
      "8 points at 55.6017325 37.597667\n",
      "11 points at 55.896612 37.398674\n",
      "12 points at 55.7344075 37.8175935\n",
      "17 points at 55.736247 37.476405\n",
      "9 points at 55.929383 37.844581\n",
      "19 points at 55.893155 37.621501\n",
      "\n",
      "Query processing took 6.06398105621 seconds\n",
      "Query: заново или заново\n",
      "Query: заново или заново\n",
      "159 points passed preprocessing\n",
      "\n",
      "Query processing took 0.922755002975 seconds\n",
      "Query: Лето - не подведи !\n",
      "Query: Знаете...у меня есть родственница. У которой сын умер от рака в 21 год. Вот она была на грани сумасшествия. Она год была клиентом психиатра.\n",
      "Query: как имя какого-то эльфа блять из Средиземья или что-то вроде\n",
      "Query: А вот и мы- творческая мастерская \"Глазурь\" @tmglazur \n",
      "Всем здравствуйте!\n",
      "Вас приветствуют две Анны.… https://t.co/RCAmyTfv9m\n",
      "Query: (все происходящее в этом фильме)\n",
      "Query: @Atl_Aztecs У Илюхи Фогга.\n",
      "Query: я снова включила клип севентин ибо уджи :D\n",
      "Query: Я ахуительная, а не охуительная. Потому что не такая как все охуительные😂\n",
      "Query: Хочу на крышу высокого здания ,просто сидеть и любоваться вечерней красотой Москвы ✨\n",
      "Query: гспд\n",
      "сейчас еще раз 10 пересмотрела\n",
      "Query: Закачал в Pocket Book около 800 книг, когда их теперь прочитать хз:))\n",
      "Query: По 22 у Икарди и Тони. В последний раз лавры лучшего в серии А разделяли два человека(Протти и Синьори) уже в далеком 1995 году!\n",
      "По 24!\n",
      "Query: @ananaskalee как бросить пить\n",
      "Query: @ananaskalee как бросить пить\n",
      "350 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 7 points among 350\n",
      "7 points at 55.759879 37.632386\n",
      "\n",
      "Query processing took 1.39730215073 seconds\n",
      "Query: В - вкусенько *0*\n",
      "Query: Солоч епта тащи #ti5ru\n",
      "Query: Солоч епта тащи #ti5ru\n",
      "466 points passed preprocessing\n",
      "\n",
      "Query processing took 1.89609098434 seconds\n",
      "Query: ладно\n",
      "Query: ладно\n",
      "2445 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 8 spatial clusters with a total of 176 points among 2445\n",
      "8 points at 55.9809335 37.1529465\n",
      "29 points at 55.809453 37.49659\n",
      "29 points at 55.328904 37.809759\n",
      "35 points at 55.599457 37.669212\n",
      "7 points at 55.821945 37.972554\n",
      "32 points at 55.891139 37.7203285\n",
      "23 points at 55.632484 37.522014\n",
      "13 points at 55.942544 37.861787\n",
      "--------------------------------------------------------------\n",
      "Found 2 temporal clusters with a total of 16 points among 2445\n",
      "7 points at 5 . 3 -- 18 : 43 : 24\n",
      "9 points at 4 . 1 -- 13 : 48 : 29\n",
      "\n",
      "Query processing took 14.5523400307 seconds\n",
      "Query: Спасибо Сашульке (sashab00 ) за такие чудесные выходные. Это были шикарные выходные в отличной… https://t.co/BXkBpEjGeW\n",
      "Query: Спасибо Сашульке (sashab00 ) за такие чудесные выходные. Это были шикарные выходные в отличной… https://t.co/BXkBpEjGeW\n",
      "379 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 11 points among 379\n",
      "11 points at 55.7526575 37.613065\n",
      "Caught exception during clusterization\n",
      "Query: @MirFOTOK На 3х коленом и медведи катаются\n",
      "Query: Они вечно там собачатся! Зачем вообще придумывают программы-передачи где все друг друга обсирают! И как там только до драки не доходит!?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sapi = tweepy.streaming.Stream(auths[2], CustomStreamListener())\n",
    "sapi.filter(locations=[left, down, right, up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = ['123123', '123', '23443']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
