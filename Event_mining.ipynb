{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import tweepy\n",
    "import random\n",
    "import urllib2\n",
    "import Stemmer\n",
    "import sqlite3\n",
    "import operator\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import stdout\n",
    "from lshash import LSHash\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Hyperion():\n",
    "    def __init__(self, data):\n",
    "        print 'Invoking Hyperion...'\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        self.probe = []\n",
    "        self.data = data\n",
    "        \n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def preprocess(self):\n",
    "        \n",
    "        print 'Preprocessing...this will take a while'\n",
    "        t0 = time.time()\n",
    "        terms = []\n",
    "        words = []\n",
    "\n",
    "        n = len(self.data.index)\n",
    "\n",
    "        for i in range(n):\n",
    "            terms.append(self.processContents(self.data.content_lower[i]))\n",
    "\n",
    "        self.data['terms'] = terms[:]\n",
    "\n",
    "        for i in range(n):\n",
    "            words += terms[i]\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "        interestingVocab = [x[0] for x in sortedDist]\n",
    "\n",
    "        #Find TF-IDF\n",
    "\n",
    "        trainingList = []\n",
    "        for i in range(n):\n",
    "            trainingList.append(' '.join(self.data['terms'][i]))\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "        self.tfidf = self.vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization\n",
    "\n",
    "        print 'Building vocab and TF-IDF matrix took', time.time()-t0, 'seconds'\n",
    "        print 'Vocab Length =', len(interestingVocab), '    Vector dimensionality =', n\n",
    "        return(self.vectorizer, self.tfidf)\n",
    "\n",
    "\n",
    "    def progress(self, i, n):\n",
    "        stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "        stdout.flush()\n",
    "        if i == n-1:\n",
    "            stdout.write(\"\\r100%\")\n",
    "            print(\"\\r\\n\")\n",
    "        \n",
    "    def median(self,lst):\n",
    "        return np.median(np.array(lst))\n",
    "\n",
    "    def extract_urls(self, lst):\n",
    "        urls = []\n",
    "        for i in lst:\n",
    "            for j in i.split(' '):\n",
    "                if j.startswith('http'):\n",
    "                    urls.append(j)\n",
    "        return urls\n",
    "\n",
    "    def resolve_url(self, starturl):\n",
    "        try:\n",
    "            req = urllib2.Request(starturl)\n",
    "            res = urllib2.urlopen(req, timeout = 2)\n",
    "            finalurl = res.geturl()\n",
    "            return finalurl\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def setup(self, vectorizer, tf_idf):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tfidf = tf_idf\n",
    "        print 'Setup complete '\n",
    "        \n",
    "\n",
    "    def findSimilarTweets(self, queryTweet, threshold, maxNumber = 0, log = False):\n",
    "\n",
    "        processedTweet = ' '.join(self.processContents(queryTweet))\n",
    "        queryTweetRepresentation = self.vectorizer.transform([processedTweet])\n",
    "\n",
    "        cosine_similarities = cosine_similarity(queryTweetRepresentation, self.tfidf)[0]\n",
    "        totalMatchingTweets = len(cosine_similarities[cosine_similarities>threshold])\n",
    "\n",
    "        if maxNumber:\n",
    "            totalMatchingTweets = min(totalMatchingTweets, maxNumber)\n",
    "        indices = cosine_similarities.argsort()[::-1][:totalMatchingTweets]\n",
    "        if len(indices) > 25:\n",
    "            print 'Query:', queryTweet\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def filter(self, x, y, tid):\n",
    "        z = filter(lambda a: a[0] != (55.753301, 37.619899), zip(zip(x,y), tid))\n",
    "        x = []\n",
    "        y = []\n",
    "        tid = []\n",
    "        for t in z:\n",
    "            x.append(t[0][0])\n",
    "            y.append(t[0][1])\n",
    "            tid.append(t[1])\n",
    "        crds = zip(x,y)\n",
    "        return crds, tid\n",
    "    \n",
    "    def performClusterisation(self, indices, thSp, thTm):\n",
    "        \n",
    "        #Spatial Clustering\n",
    "        y = list(self.data['lat'][indices])\n",
    "        x = list(self.data['long'][indices])\n",
    "        tid = indices\n",
    "        \n",
    "        crds, tid = self.filter(x, y, tid)\n",
    "\n",
    "        spUniques = self.findDBCluster(crds, tid, thSp, 5)\n",
    "        \n",
    "        if len(spUniques):\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', len(spUniques), 'spatial clusters'\n",
    "\n",
    "        for w in spUniques:\n",
    "            kw = ' '.join(self.extractKeywords(w['ids'])[:5])\n",
    "            print len(w['ids']), 'points at', w['xm'], w['ym'], kw\n",
    "\n",
    "\n",
    "        #Temporal Clustering\n",
    "        tmvc = []\n",
    "        for t in self.data['created_at'][indices]:\n",
    "            l = t.split(' ')\n",
    "            date = l[0].split('-')\n",
    "            time = l[1].split(':')\n",
    "            datehash = int(date[1]) * 30 + int(date[2])\n",
    "            timehash = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
    "            tmvc.append((datehash*400, timehash))\n",
    "\n",
    "        tpUniques = self.findDBCluster(tmvc, tid, thTm, 5)\n",
    "\n",
    "        if len(tpUniques):\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', len(tpUniques), 'temporal clusters'\n",
    "        \n",
    "\n",
    "        for w in tpUniques:\n",
    "            date = w['xm']/400\n",
    "            time = w['ym']\n",
    "            kw = ' '.join(self.extractKeywords(w['ids'])[:5])\n",
    "            print len(w['ids']), 'points at', int(date//30),'.',int(date%30),'--',int(time//3600),\n",
    "            print ':',int(time%3600//60),':',int(time%3600%60), kw\n",
    "\n",
    "        print ''\n",
    "        return ((spUniques, tpUniques))\n",
    "    \n",
    "    def extractKeywords(self, cluster):\n",
    "        keywords = []\n",
    "        corpus = ' '.join(list(self.data['content'][cluster]))\n",
    "        corpus = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', corpus)\n",
    "        words = []\n",
    "        tokenized = nltk.tokenize.word_tokenize(corpus)\n",
    "        for word in tokenized:\n",
    "            words.append(word)\n",
    "        #words = self.stemmer.stemWords(words)\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        moreThan3 = [x for x in sortedDist if len(x[0]) > 3 and x[1] > 3]\n",
    "        for i in moreThan3:\n",
    "            keywords.append(i[0])\n",
    "\n",
    "        return keywords\n",
    "    \n",
    "    def processURLs(self, cluster):\n",
    "        insta = []\n",
    "        swarm = []\n",
    "        redirects = []\n",
    "        lst = list(self.data['content'][cluster])\n",
    "        urls = self.extract_urls(lst)\n",
    "        for i in urls:\n",
    "            redirects.append(self.resolve_url(i))\n",
    "\n",
    "        print 'Found', len(redirects), 'anonymous links, investigating...'\n",
    "        for i, u in enumerate(redirects):\n",
    "            if u:\n",
    "                if 'instagram' in u:\n",
    "                    insta.append(u)\n",
    "                if 'swarmapp' in u:\n",
    "                    swarm.append(u)\n",
    "\n",
    "        return (insta, swarm)\n",
    "\n",
    "    def findDBCluster(self, crds, tids, epsilon, min):\n",
    "\n",
    "        clusters = []\n",
    "        rez = []\n",
    "        X = np.array(crds)\n",
    "        db = DBSCAN(eps=epsilon, min_samples=min).fit(X)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        unique_labels = set(labels)\n",
    "        colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:\n",
    "                # Black used for noise.\n",
    "                col = 'k'\n",
    "\n",
    "            class_member_mask = (labels == k)\n",
    "\n",
    "            xy = X[class_member_mask & core_samples_mask]\n",
    "            #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=10)\n",
    "            if len(xy):\n",
    "                rez.append(xy)\n",
    "            xy = X[class_member_mask & ~core_samples_mask]\n",
    "            #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col, markeredgecolor='k', markersize=2)\n",
    "\n",
    "        #plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "        #plt.show()\n",
    "\n",
    "        u = zip (crds, tids)\n",
    "        for c in rez:\n",
    "            buf = []\n",
    "            for t in c:\n",
    "                for j in u:\n",
    "                    if tuple(t) == j[0]:\n",
    "                        buf.append(j)\n",
    "            clusters.append(list(set(buf)))\n",
    "\n",
    "        scored = []\n",
    "        for c in clusters:\n",
    "            scored.append(self.score(c))\n",
    "\n",
    "        scored = filter(lambda a: len(a['ids'])> min, scored)\n",
    "        scored.sort(key = lambda t: t['score'], reverse = True)\n",
    "\n",
    "        return scored\n",
    "    \n",
    "    def score(self, cluster):\n",
    "        processed = {'ids':[], 'xm':0, 'ym':0, 'score':0, 'num':0}\n",
    "        cluster = list(set(cluster))\n",
    "        if len(cluster):\n",
    "            xs = [t[0][0] for t in cluster]\n",
    "            ys = [t[0][1] for t in cluster]\n",
    "            ids = [t[1] for t in cluster]\n",
    "            \n",
    "            processed['ids'] = ids\n",
    "            processed['xm'] = hyperion.median(xs)\n",
    "            processed['ym'] = hyperion.median(ys)\n",
    "            processed['score'] = len(ids)*len(ids)/((max(xs)-min(xs)+0.01)*(max(ys)-min(ys)+0.01))\n",
    "            processed['num'] = len(ids)\n",
    "            \n",
    "        return processed\n",
    "\n",
    "    def doQuery(self, query, th_NN, th_SP, th_TM):\n",
    "        t0 = time.time()\n",
    "        indices = self.findSimilarTweets(query, th_NN)\n",
    "        print len(indices), 'points passed preprocessing'\n",
    "\n",
    "        if len(indices):\n",
    "            t = self.performClusterisation(indices, th_SP, th_TM)\n",
    "            print \"Query processing took\", time.time()-t0, 'seconds'\n",
    "\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "con = sqlite3.connect('tweetsSpring.db')\n",
    "data = pd.read_sql('select * from tweets', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Setup complete \n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)\n",
    "hyperion.setup(vec, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: я в музее на выставке искусств\n",
      "479 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 7 spatial clusters\n",
      "116 points at 55.736267 37.607722 Музеон Искусств Парк музеон moscow\n",
      "133 points at 55.75009 37.600264 Музей искусства музей Москва Московский\n",
      "18 points at 55.823226 37.639813 космонавтики Музей\n",
      "12 points at 55.691089 37.561791 музей Дарвиновский Государственный дарвиновскиймузей\n",
      "17 points at 55.774526 37.611161 Музей Искусства Декоративно-Прикладного выставка метроивойна\n",
      "14 points at 55.751917 37.668921 Наука искусство Выставка Искусство\n",
      "6 points at 55.772455 37.6689815 Зверевский искусства\n",
      "--------------------------------------------------------------\n",
      "Found 8 temporal clusters\n",
      "80 points at 5 . 14 -- 16 : 56 : 30 Музеон Искусств Парк Музей искусства\n",
      "33 points at 5 . 16 -- 20 : 26 : 54 Музей Искусств Музеон Парк Современного\n",
      "25 points at 4 . 21 -- 18 : 19 : 9 Искусств Музеон Парк музей Государственный\n",
      "8 points at 4 . 22 -- 10 : 39 : 15 искусство\n",
      "9 points at 4 . 11 -- 14 : 23 : 6 музей\n",
      "17 points at 4 . 22 -- 12 : 30 : 44 Москва музей искусства современного Искусств\n",
      "14 points at 4 . 21 -- 15 : 59 : 44 музей\n",
      "17 points at 5 . 11 -- 13 : 3 : 0 Музей Музеон Парк Искусств\n",
      "\n",
      "Query processing took 0.800000190735 seconds\n"
     ]
    }
   ],
   "source": [
    "u = hyperion.doQuery(u'я в музее на выставке искусств', 0.34, 0.007, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Preprocessing...this will take a while\n",
      "Building vocab and TF-IDF matrix took 128.057999849 seconds\n",
      "Vocab Length = 414550     Vector dimensionality = 1315775\n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)\n",
    "vec, tf = hyperion.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findTSCluster(u):\n",
    "    w = []\n",
    "    tsc = []\n",
    "    th = 0.2\n",
    "    for a in u[0]:\n",
    "        for b in u[1]:\n",
    "            if jaccard_similarity(a['ids'], b['ids']) > th: \n",
    "                tsc = a['ids'] + b['ids']\n",
    "                w.append(tsc)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(a, b):\n",
    "    s = 0\n",
    "    t = []\n",
    "    for i in a:\n",
    "        if i in b:\n",
    "            s += 1\n",
    "    for i in b:\n",
    "        if i in a:\n",
    "            s += 1\n",
    "    l = float(len(a) + len(b))\n",
    "    return float(s/l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Музеон\n",
      "Искусств\n",
      "Парк\n",
      "Музей\n",
      "музеон\n",
      "Москва\n",
      "искусства\n",
      "Moscow\n",
      "музей\n",
      "moscow\n",
      "современного\n",
      "москва\n",
      "Искусства\n",
      "выставка\n",
      "Москвы\n",
      "искусство\n",
      "spring\n",
      "Московский\n",
      "деньпобеды\n",
      "love\n",
      "цепи\n",
      "Museum\n",
      "Современного\n",
      "паркгорького\n",
      "Сегодня\n"
     ]
    }
   ],
   "source": [
    "for i in hyperion.extractKeywords(w[0]):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w=findTSCluster(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
