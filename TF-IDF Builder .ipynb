{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "#Getting the data\n",
    "con = sqlite3.connect('tweetsSpring.db')\n",
    "data = pd.read_sql(\"SELECT * from tweets where month = 5 and day != 26\",con)\n",
    "dataLast = pd.read_sql(\"SELECT * from tweets where month = 5 and day = 26\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Progress\n",
    "from sys import stdout\n",
    "def progress(i, n):\n",
    "    stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "    stdout.flush()\n",
    "    if i == n-1:\n",
    "        stdout.write(\"\\r100%\")\n",
    "        print(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Processor...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pymorphy2\n",
    "import Stemmer\n",
    "\n",
    "class TweetTextParser():\n",
    "\n",
    "    def __init__(self):\n",
    "        print 'Invoking Processor...'\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        try:\n",
    "            self.emo_db = json.load(open('pyalchemy/emoji_database','r'))\n",
    "        except:\n",
    "            print('No emoji database found')\n",
    "\n",
    "\n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def resolveEmoji(self, myText):\n",
    "        emostr = []\n",
    "        emo_db = self.emo_db\n",
    "        b = myText.encode('unicode_escape').split('\\\\')\n",
    "        c = [point.replace('000','+').upper() for point in b if len(point) > 8 and point[0] == 'U']\n",
    "        [emostr.append(emo_db[emo[:7]]) for emo in c if emo[:7] in emo_db]\n",
    "        return myText\n",
    "\n",
    "textProcessor = TweetTextParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.3677601814\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "#Cleaning up the data\n",
    "t0 = time()\n",
    "terms = []\n",
    "n = len(data.index)\n",
    "#n = 100000\n",
    "for i in range(n):\n",
    "    terms.append(textProcessor.processContents(data.content_lower[i]))\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['terms'] = terms[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ну\n",
      "кто\n",
      "ещ\n",
      "попадет\n",
      "на\n",
      "инвентаризац\n",
      "есл\n",
      "не\n",
      "я\n",
      "пожалуйст\n",
      "пуст\n",
      "утр\n",
      "все\n",
      "эт\n",
      "окажет\n",
      "дурн\n",
      "сном\n",
      "Смотр\n",
      "я\n",
      "в\n",
      "Москв\n",
      "Сейчас\n",
      "9\n",
      "ясн\n",
      "Утром\n",
      "7\n",
      "облачн\n",
      "небольш\n",
      "дожд\n",
      "Днем\n",
      "11\n",
      "пасмурн\n",
      "дожд\n",
      "Вечер\n",
      "Аа\n",
      "ты\n",
      "в\n",
      "Москв\n",
      "Немн\n",
      "приятн\n",
      "момент\n",
      "Ну\n",
      "взяв\n",
      "тво\n",
      "ручк\n",
      "в\n",
      "сво\n",
      "несильн\n",
      "сжима\n",
      "да\n",
      "да\n",
      "я\n",
      "в\n",
      "Москв\n",
      "Росс\n",
      "ахаххах\n",
      "аргументн\n",
      "аргумент\n"
     ]
    }
   ],
   "source": [
    "for row in data.index[:10]: \n",
    "    for word in data['terms'][row]: print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  262391\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "words = []\n",
    "for i in range(n):\n",
    "    words += terms[i]\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "#allTheWords = [x for x in moreThan3 if x[1] > 1]\n",
    "interestingVocab = [x[0] for x in sortedDist]\n",
    "print 'Vocab Length: ', len(interestingVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "что\n",
      "так\n",
      "как\n",
      "Москв\n",
      "мен\n",
      "все\n",
      "мне\n",
      "был\n",
      "теб\n",
      "прост\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print interestingVocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "trainingList = []\n",
    "for i in range(n):\n",
    "    trainingList.append(' '.join(data['terms'][i]))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def findSimilarTweets(queryTweet, data, threshold, maxNumber = 0, log = True):\n",
    "    print 'Query:', queryTweet\n",
    "    \n",
    "    processedTweet = ' '.join(textProcessor.processContents(queryTweet))\n",
    "    queryTweetRepresentation = tfidf_vectorizer.transform([processedTweet])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cosine_similarities = cosine_similarity(queryTweetRepresentation, tfidf_matrix_train)[0]\n",
    "    totalMatchingTweets = len(cosine_similarities[cosine_similarities>threshold])\n",
    "    if maxNumber:\n",
    "        totalMatchingTweets = min(totalMatchingTweets, maxNumber)\n",
    "    indices = cosine_similarities.argsort()[::-1][:totalMatchingTweets]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if (log):\n",
    "        print 'time: ', elapsed_time\n",
    "        print ''\n",
    "        print 'Results:'\n",
    "        print data[indices]\n",
    "        print ''\n",
    "        print 'cosine scores ==>', cosine_similarities[indices]\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: в метро объявляют станции\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "inidiciesOfTweets = findSimilarTweets(u'в метро объявляют станции', data['content'], 0.5, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "data['date'] = [datetime.strptime(data.created_at[i], \"%Y-%m-%d %H:%M:%S\").date() for i in data.index]\n",
    "data['time'] = [datetime.strptime(data.created_at[i], \"%Y-%m-%d %H:%M:%S\").time() for i in data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  15268\n"
     ]
    }
   ],
   "source": [
    "print 'Vocab Length: ', len(interestingVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ребята, мы всем рады!\n"
     ]
    }
   ],
   "source": [
    "print data['cleanText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673685"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainigList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f4577b5aa7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m lsh = LSH( 100,\n\u001b[0;32m----> 6\u001b[0;31m            \u001b[0mtfidf_matrix_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m            \u001b[0mnum_hashtables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m            storage_config={\"dict\":None})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_matrix_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sparselsh import LSH\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "t0 = time()\n",
    "lsh = LSH( 100,\n",
    "           tfidf_matrix_train.shape[1],\n",
    "           num_hashtables=1,\n",
    "           storage_config={\"dict\":None})\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.470348%"
     ]
    }
   ],
   "source": [
    "for ix in xrange(tfidf_matrix_train.shape[0]):\n",
    "    x = tfidf_matrix_train.getrow(ix)\n",
    "    lsh.index(x)\n",
    "    progress(ix,tfidf_matrix_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMedian(lst):\n",
    "    import numpy\n",
    "    return numpy.median(numpy.array(lst))\n",
    "\n",
    "def checkSpatialDensity(indices):\n",
    "    \n",
    "    lats = filter(lambda a: a != 37.619899, data['lat'][indices])\n",
    "    lons = filter(lambda a: a != 55.753301, data['long'][indices])\n",
    "    nf = len(lats)\n",
    "    \n",
    "    ltm = median(lats)\n",
    "    lns = median(lons)\n",
    "    \n",
    "    cluster = []\n",
    "    for i in indices:\n",
    "        if data['lat'][i] == '37.619899' and data['long'][i] == '55.753301':\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            dist = (data['lat'][i]-ltm)*(data['lat'][i]-ltm) + (data['long'][i]-lns)*(data['long'][i]-lns)\n",
    "            if dist < 0.0001:\n",
    "                cluster.append(i)\n",
    "                \n",
    "    print nf, len(cluster)\n",
    "            \n",
    "    if len(cluster) > 0.07 * nf:\n",
    "        x = median(filter(lambda a: a != 37.619899, data['lat'][cluster]))\n",
    "        y = median(filter(lambda a: a != 55.753301, data['long'][cluster]))\n",
    "        print 'Found a spatial cluster with', len(cluster), 'points', y, x\n",
    "    else:\n",
    "        print 'No spatial clustering detected'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def median(lst):\n",
    "    import numpy\n",
    "    return numpy.median(numpy.array(lst))\n",
    "\n",
    "def checkSpatialDensity(indices):\n",
    "    \n",
    "    lats = filter(lambda a: a != 37.619899, data['lat'][indices])\n",
    "    lons = filter(lambda a: a != 55.753301, data['long'][indices])\n",
    "    nf = len(lats)\n",
    "    \n",
    "    ltm = median(lats)\n",
    "    lns = median(lons)\n",
    "    \n",
    "    cluster = []\n",
    "    for i in indices:\n",
    "        if data['lat'][i] == '37.619899' and data['long'][i] == '55.753301':\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            dist = (data['lat'][i]-ltm)*(data['lat'][i]-ltm) + (data['long'][i]-lns)*(data['long'][i]-lns)\n",
    "            if dist < 0.0001:\n",
    "                cluster.append(i)\n",
    "            \n",
    "    if len(cluster) > 0.07 * nf:\n",
    "        x = median(filter(lambda a: a != 37.619899, data['lat'][cluster]))\n",
    "        y = median(filter(lambda a: a != 55.753301, data['long'][cluster]))\n",
    "        print 'Found a spatial cluster with', len(cluster), 'points', y, x\n",
    "    else:\n",
    "        print 'No spatial clustering detected'\n",
    "        \n",
    "def checkTimeDensity(indices):\n",
    "    cluster = []\n",
    "    days = []\n",
    "    times = []\n",
    "    \n",
    "    for item in indices:\n",
    "        days.append(int(data['created_at'][item].split(' ')[0].split('-')[2]))\n",
    "        times.append(float(data['created_at'][item].split(' ')[1][0:5].replace(':','.')))\n",
    "        \n",
    "    tm = median(times)\n",
    "    dm = median(days)\n",
    "\n",
    "    for item in indices:\n",
    "        day = int(data['created_at'][item].split(' ')[0].split('-')[2])\n",
    "        time = float(data['created_at'][item].split(' ')[1][0:5].replace(':','.'))\n",
    "        if abs(day - dm) < 3:\n",
    "            if abs(time - tm) < 3:\n",
    "                cluster.append(item)\n",
    "                \n",
    "    for item in cluster:\n",
    "        days.append(int(data['created_at'][item].split(' ')[0].split('-')[2]))\n",
    "        times.append(float(data['created_at'][item].split(' ')[1][0:5].replace(':','.')))\n",
    "    \n",
    "    tm = str(median(times)).replace('.',':')\n",
    "    dm = int(median(days))\n",
    "                \n",
    "    print 'Found a temporal cluster with', len(cluster), 'points', dm, tm\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Bosco\n",
      "Found a spatial cluster with 47 points 55.77115 37.609414\n",
      "Found a temporal cluster with 35 points 23 16:01\n"
     ]
    }
   ],
   "source": [
    "inidiciesOfTweets = findSimilarTweets(u'Bosco', data['content'], 0.5, log = False)\n",
    "checkSpatialDensity(inidiciesOfTweets)\n",
    "checkTimeDensity(inidiciesOfTweets)\n",
    "neighBoors = data.loc[inidiciesOfTweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384598    37.617680\n",
       "82023     37.620712\n",
       "157464    37.621531\n",
       "634921    37.609414\n",
       "590183    37.609414\n",
       "585592    37.609414\n",
       "611062    37.609414\n",
       "632540    37.609414\n",
       "618782    37.609414\n",
       "580885    37.609414\n",
       "585457    37.609414\n",
       "629721    37.609414\n",
       "634571    37.609414\n",
       "583844    37.609414\n",
       "628873    37.609414\n",
       "583655    37.609414\n",
       "618592    37.609414\n",
       "618873    37.609414\n",
       "586286    37.608567\n",
       "583816    37.608276\n",
       "579099    37.610418\n",
       "587091    37.608567\n",
       "582297    37.608754\n",
       "582684    37.608626\n",
       "586402    37.608567\n",
       "582372    37.608654\n",
       "582966    37.609414\n",
       "588853    37.609377\n",
       "583993    37.609414\n",
       "588683    37.609414\n",
       "588694    37.609414\n",
       "636654    37.609414\n",
       "635104    37.609414\n",
       "586753    37.609414\n",
       "629333    37.609414\n",
       "584665    37.609414\n",
       "640449    37.609414\n",
       "635453    37.609414\n",
       "634036    37.608228\n",
       "633603    37.608228\n",
       "631684    37.608228\n",
       "633380    37.608228\n",
       "628816    37.608299\n",
       "626208    37.608828\n",
       "633608    37.608228\n",
       "626875    37.608299\n",
       "631772    37.608228\n",
       "587086    37.609414\n",
       "583212    37.609414\n",
       "599490    37.584960\n",
       "626892    37.609414\n",
       "Name: lat, dtype: float64"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighBoors.lat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
