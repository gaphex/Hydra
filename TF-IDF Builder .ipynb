{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import collections\n",
    "%matplotlib inline\n",
    "\n",
    "#Getting the data\n",
    "con = sqlite3.connect('tweetsSpring.db')\n",
    "data = pd.read_sql(\"SELECT * from tweets where month = 5 and day != 26\",con)\n",
    "dataLast = pd.read_sql(\"SELECT * from tweets where month = 5 and day = 26\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Progress\n",
    "from sys import stdout\n",
    "def progress(i, n):\n",
    "    stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "    stdout.flush()\n",
    "    if i == n-1:\n",
    "        stdout.write(\"\\r100%\")\n",
    "        print(\"\\r\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Processor...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pymorphy2\n",
    "import Stemmer\n",
    "\n",
    "class TweetTextParser():\n",
    "\n",
    "    def __init__(self):\n",
    "        print 'Invoking Processor...'\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        try:\n",
    "            self.emo_db = json.load(open('pyalchemy/emoji_database','r'))\n",
    "        except:\n",
    "            print('No emoji database found')\n",
    "\n",
    "\n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def resolveEmoji(self, myText):\n",
    "        emostr = []\n",
    "        emo_db = self.emo_db\n",
    "        b = myText.encode('unicode_escape').split('\\\\')\n",
    "        c = [point.replace('000','+').upper() for point in b if len(point) > 8 and point[0] == 'U']\n",
    "        [emostr.append(emo_db[emo[:7]]) for emo in c if emo[:7] in emo_db]\n",
    "        return myText\n",
    "\n",
    "textProcessor = TweetTextParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.3677601814\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "#Cleaning up the data\n",
    "t0 = time()\n",
    "terms = []\n",
    "n = len(data.index)\n",
    "#n = 100000\n",
    "for i in range(n):\n",
    "    terms.append(textProcessor.processContents(data.content_lower[i]))\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['terms'] = terms[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.tail of 0         2015-05-01 00:00:33\n",
      "1         2015-05-01 00:00:41\n",
      "2         2015-05-01 00:00:46\n",
      "3         2015-05-01 00:01:00\n",
      "4         2015-05-01 00:01:07\n",
      "5         2015-05-01 00:01:10\n",
      "6         2015-05-01 00:01:21\n",
      "7         2015-05-01 00:01:28\n",
      "8         2015-05-01 00:01:44\n",
      "9         2015-05-01 00:02:01\n",
      "10        2015-05-01 00:02:09\n",
      "11        2015-05-01 00:02:13\n",
      "12        2015-05-01 00:02:16\n",
      "13        2015-05-01 00:02:51\n",
      "14        2015-05-01 00:02:59\n",
      "15        2015-05-01 00:03:08\n",
      "16        2015-05-01 00:03:24\n",
      "17        2015-05-01 00:03:32\n",
      "18        2015-05-01 00:03:50\n",
      "19        2015-05-01 00:03:57\n",
      "20        2015-05-01 00:04:03\n",
      "21        2015-05-01 00:04:23\n",
      "22        2015-05-01 00:04:30\n",
      "23        2015-05-01 00:04:31\n",
      "24        2015-05-01 00:04:34\n",
      "25        2015-05-01 00:04:35\n",
      "26        2015-05-01 00:04:41\n",
      "27        2015-05-01 00:04:57\n",
      "28        2015-05-01 00:05:01\n",
      "29        2015-05-01 00:05:03\n",
      "                 ...         \n",
      "667468    2015-05-25 23:56:09\n",
      "667469    2015-05-25 23:56:11\n",
      "667470    2015-05-25 23:56:15\n",
      "667471    2015-05-25 23:56:19\n",
      "667472    2015-05-25 23:56:30\n",
      "667473    2015-05-25 23:56:41\n",
      "667474    2015-05-25 23:56:50\n",
      "667475    2015-05-25 23:56:51\n",
      "667476    2015-05-25 23:56:58\n",
      "667477    2015-05-25 23:56:58\n",
      "667478    2015-05-25 23:57:02\n",
      "667479    2015-05-25 23:57:14\n",
      "667480    2015-05-25 23:57:19\n",
      "667481    2015-05-25 23:57:36\n",
      "667482    2015-05-25 23:57:41\n",
      "667483    2015-05-25 23:57:41\n",
      "667484    2015-05-25 23:57:44\n",
      "667485    2015-05-25 23:57:53\n",
      "667486    2015-05-25 23:57:54\n",
      "667487    2015-05-25 23:57:55\n",
      "667488    2015-05-25 23:58:10\n",
      "667489    2015-05-25 23:58:12\n",
      "667490    2015-05-25 23:58:26\n",
      "667491    2015-05-25 23:58:32\n",
      "667492    2015-05-25 23:58:49\n",
      "667493    2015-05-25 23:59:12\n",
      "667494    2015-05-25 23:59:17\n",
      "667495    2015-05-25 23:59:26\n",
      "667496    2015-05-25 23:59:31\n",
      "667497    2015-05-25 23:59:36\n",
      "Name: created_at, dtype: object>\n",
      "<bound method Series.tail of 0       2015-05-26 00:00:04\n",
      "1       2015-05-26 00:00:38\n",
      "2       2015-05-26 00:00:39\n",
      "3       2015-05-26 00:00:56\n",
      "4       2015-05-26 00:01:02\n",
      "5       2015-05-26 00:01:05\n",
      "6       2015-05-26 00:01:20\n",
      "7       2015-05-26 00:01:24\n",
      "8       2015-05-26 00:01:33\n",
      "9       2015-05-26 00:01:34\n",
      "10      2015-05-26 00:01:39\n",
      "11      2015-05-26 00:01:40\n",
      "12      2015-05-26 00:01:43\n",
      "13      2015-05-26 00:01:53\n",
      "14      2015-05-26 00:02:07\n",
      "15      2015-05-26 00:02:08\n",
      "16      2015-05-26 00:02:13\n",
      "17      2015-05-26 00:02:15\n",
      "18      2015-05-26 00:02:28\n",
      "19      2015-05-26 00:02:42\n",
      "20      2015-05-26 00:02:45\n",
      "21      2015-05-26 00:04:08\n",
      "22      2015-05-26 00:04:13\n",
      "23      2015-05-26 00:04:14\n",
      "24      2015-05-26 00:04:15\n",
      "25      2015-05-26 00:04:25\n",
      "26      2015-05-26 00:04:36\n",
      "27      2015-05-26 00:04:38\n",
      "28      2015-05-26 00:04:39\n",
      "29      2015-05-26 00:04:40\n",
      "               ...         \n",
      "6157    2015-05-26 10:02:02\n",
      "6158    2015-05-26 10:02:03\n",
      "6159    2015-05-26 10:02:05\n",
      "6160    2015-05-26 10:02:06\n",
      "6161    2015-05-26 10:02:06\n",
      "6162    2015-05-26 10:02:08\n",
      "6163    2015-05-26 10:02:11\n",
      "6164    2015-05-26 10:02:11\n",
      "6165    2015-05-26 10:02:12\n",
      "6166    2015-05-26 10:02:14\n",
      "6167    2015-05-26 10:02:15\n",
      "6168    2015-05-26 10:02:20\n",
      "6169    2015-05-26 10:02:24\n",
      "6170    2015-05-26 10:02:24\n",
      "6171    2015-05-26 10:02:27\n",
      "6172    2015-05-26 10:02:27\n",
      "6173    2015-05-26 10:02:32\n",
      "6174    2015-05-26 10:02:38\n",
      "6175    2015-05-26 10:02:38\n",
      "6176    2015-05-26 10:02:39\n",
      "6177    2015-05-26 10:02:40\n",
      "6178    2015-05-26 10:02:41\n",
      "6179    2015-05-26 10:02:42\n",
      "6180    2015-05-26 10:02:44\n",
      "6181    2015-05-26 10:02:44\n",
      "6182    2015-05-26 10:02:48\n",
      "6183    2015-05-26 10:02:52\n",
      "6184    2015-05-26 10:02:53\n",
      "6185    2015-05-26 10:02:54\n",
      "6186    2015-05-26 10:02:54\n",
      "Name: created_at, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "print data.created_at.tail\n",
    "print dataLast.created_at.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù—É\n",
      "–∫—Ç–æ\n",
      "–µ—â\n",
      "–ø–æ–ø–∞–¥–µ—Ç\n",
      "–Ω–∞\n",
      "–∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü\n",
      "–µ—Å–ª\n",
      "–Ω–µ\n",
      "—è\n",
      "–ø–æ–∂–∞–ª—É–π—Å—Ç\n",
      "–ø—É—Å—Ç\n",
      "—É—Ç—Ä\n",
      "–≤—Å–µ\n",
      "—ç—Ç\n",
      "–æ–∫–∞–∂–µ—Ç\n",
      "–¥—É—Ä–Ω\n",
      "—Å–Ω–æ–º\n",
      "–°–º–æ—Ç—Ä\n",
      "—è\n",
      "–≤\n",
      "–ú–æ—Å–∫–≤\n",
      "–°–µ–π—á–∞—Å\n",
      "9\n",
      "—è—Å–Ω\n",
      "–£—Ç—Ä–æ–º\n",
      "7\n",
      "–æ–±–ª–∞—á–Ω\n",
      "–Ω–µ–±–æ–ª—å—à\n",
      "–¥–æ–∂–¥\n",
      "–î–Ω–µ–º\n",
      "11\n",
      "–ø–∞—Å–º—É—Ä–Ω\n",
      "–¥–æ–∂–¥\n",
      "–í–µ—á–µ—Ä\n",
      "–ê–∞\n",
      "—Ç—ã\n",
      "–≤\n",
      "–ú–æ—Å–∫–≤\n",
      "–ù–µ–º–Ω\n",
      "–ø—Ä–∏—è—Ç–Ω\n",
      "–º–æ–º–µ–Ω—Ç\n",
      "–ù—É\n",
      "–≤–∑—è–≤\n",
      "—Ç–≤–æ\n",
      "—Ä—É—á–∫\n",
      "–≤\n",
      "—Å–≤–æ\n",
      "–Ω–µ—Å–∏–ª—å–Ω\n",
      "—Å–∂–∏–º–∞\n",
      "–¥–∞\n",
      "–¥–∞\n",
      "—è\n",
      "–≤\n",
      "–ú–æ—Å–∫–≤\n",
      "–†–æ—Å—Å\n",
      "–∞—Ö–∞—Ö—Ö–∞—Ö\n",
      "–∞—Ä–≥—É–º–µ–Ω—Ç–Ω\n",
      "–∞—Ä–≥—É–º–µ–Ω—Ç\n"
     ]
    }
   ],
   "source": [
    "for row in data.index[:10]: \n",
    "    for word in data['terms'][row]: print word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  262391\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "words = []\n",
    "for i in range(n):\n",
    "    words += terms[i]\n",
    "\n",
    "fdist = nltk.FreqDist(words)\n",
    "sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "#allTheWords = [x for x in moreThan3 if x[1] > 1]\n",
    "interestingVocab = [x[0] for x in sortedDist]\n",
    "print 'Vocab Length: ', len(interestingVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—á—Ç–æ\n",
      "—Ç–∞–∫\n",
      "–∫–∞–∫\n",
      "–ú–æ—Å–∫–≤\n",
      "–º–µ–Ω\n",
      "–≤—Å–µ\n",
      "–º–Ω–µ\n",
      "–±—ã–ª\n",
      "—Ç–µ–±\n",
      "–ø—Ä–æ—Å—Ç\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print interestingVocab[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "trainingList = []\n",
    "for i in range(n):\n",
    "    trainingList.append(' '.join(data['terms'][i]))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def findSimilarTweets(queryTweet, data, threshhold):\n",
    "    print 'Query:', queryTweet\n",
    "    processedTweet = ' '.join(textProcessor.processContents(queryTweet))\n",
    "    \n",
    "    queryTweetRepresentation = tfidf_vectorizer.transform([processedTweet])\n",
    "    start_time = time.time()\n",
    "    cosine_similarities = cosine_similarity(queryTweetRepresentation, tfidf_matrix_train)[0]\n",
    "    nearest_objects = cosine_similarities[cosine_similarities>0.4]\n",
    "    indices = nearest_objects.argsort()[::-1]\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print 'time: ', elapsed_time\n",
    "    \n",
    "    print 'cosine scores ==>', nearest_objects[indices]\n",
    "    print ''\n",
    "    print 'Results:'\n",
    "    print data[indices]\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: –≤ –º–µ—Ç—Ä–æ –æ–±—ä—è–≤–ª—è—é—Ç —Å—Ç–∞–Ω—Ü–∏–∏\n",
      "time:  0.217649936676\n",
      "cosine scores ==> [ 0.75964594  0.73711167  0.71805737  0.71462365  0.69707083  0.69212216\n",
      "  0.68498465  0.6784575   0.67471516  0.6738558   0.67147243  0.66798734\n",
      "  0.66493821  0.65294042  0.6516734   0.64886296  0.6468789   0.63383193\n",
      "  0.63178933  0.6277312   0.62761031  0.62701111  0.62164232  0.61345563\n",
      "  0.61131433  0.59158023  0.59102898  0.58514276  0.58495025  0.58495025\n",
      "  0.58495025  0.58495025  0.58495025  0.58495025  0.58495025  0.58495025\n",
      "  0.58495025  0.58495025  0.58495025  0.58495025  0.57897889  0.57746253\n",
      "  0.57470785  0.57227371  0.56643359  0.5628667   0.56256038  0.56235789\n",
      "  0.55884245  0.55828391  0.55544978  0.55544978  0.55544978  0.55544978\n",
      "  0.54969781  0.54801253  0.54638102  0.54635907  0.54594384  0.54500748\n",
      "  0.54233213  0.54202074  0.54202074  0.54202074  0.54202074  0.54202074\n",
      "  0.54202074  0.54202074  0.53763038  0.53629846  0.53614752  0.535267\n",
      "  0.52784001  0.52716108  0.52587483  0.52514647  0.52266729  0.52203167\n",
      "  0.52036349  0.51672134  0.51629726  0.51629726  0.51629726  0.51629726\n",
      "  0.51629726  0.51629726  0.51629726  0.51572596  0.51572596  0.51306619\n",
      "  0.51149356  0.5106674   0.50638231  0.5061847   0.50525644  0.50082301\n",
      "  0.49773356  0.49715277  0.49698264  0.49652018  0.49644546  0.49644546\n",
      "  0.49597995  0.49283711  0.492439    0.49203058  0.49144353  0.4873524\n",
      "  0.48557801  0.48555204  0.48224654  0.48140929  0.47905075  0.47743481\n",
      "  0.47676409  0.47336218  0.47241264  0.47221428  0.4720211   0.4720211\n",
      "  0.4720211   0.4720211   0.4720211   0.4720211   0.4720211   0.4720211\n",
      "  0.4720211   0.4720211   0.4720211   0.4720211   0.4720211   0.4720211\n",
      "  0.4720211   0.4720211   0.4720211   0.4720211   0.4720211   0.4720211\n",
      "  0.4720211   0.4720211   0.4720211   0.4720211   0.4720211   0.47198446\n",
      "  0.47024142  0.46952861  0.46857745  0.4681906   0.4681906   0.4681906\n",
      "  0.46806349  0.46741847  0.46701376  0.46603978  0.46557348  0.46537854\n",
      "  0.46343626  0.46329787  0.46312266  0.46294244  0.4622992   0.46136901\n",
      "  0.45991975  0.45958891  0.45921353  0.45790186  0.45651549  0.45644062\n",
      "  0.45599483  0.45530253  0.45227062  0.44967872  0.44876241  0.44871473\n",
      "  0.44809825  0.44590318  0.44576392  0.44539577  0.44278169  0.44276227\n",
      "  0.4419872   0.44123705  0.44115214  0.44106655  0.43914674  0.43909636\n",
      "  0.43877082  0.43842831  0.43773985  0.43717496  0.43642606  0.43612176\n",
      "  0.43590945  0.4355967   0.43537097  0.43495559  0.43488663  0.43482462\n",
      "  0.43287118  0.43238638  0.43199301  0.43136224  0.43082064  0.42965834\n",
      "  0.42905058  0.42903768  0.4270163   0.42610327  0.42506924  0.42421102\n",
      "  0.42360439  0.42189269  0.42182808  0.42124773  0.42078312  0.42031067\n",
      "  0.41964964  0.4188624   0.41873288  0.41403893  0.41265036  0.41265036\n",
      "  0.41244171  0.4122861   0.41192536  0.41139509  0.41055586  0.41040206\n",
      "  0.41036766  0.41001952  0.4098226   0.40887983  0.40886636  0.40844726\n",
      "  0.40755527  0.40687479  0.40644543  0.40427859  0.40355055  0.40280155\n",
      "  0.40268896  0.40230118  0.40056229  0.40053436  0.40037854  0.40037854\n",
      "  0.40037854]\n",
      "\n",
      "Results:\n",
      "122    @David_Oakes You always find a risky role. Dav...\n",
      "49                                 @Jelenatorrrr –ø–æ—á–µ–º—É?\n",
      "32     @24_7_Juan_Kerr @CuckoldingCom http://t.co/6gt...\n",
      "62     –ü—É—Ç–µ—à–µ—Å—Ç–≤–∏—è –ø—Ä–æ–¥–æ–ª–∂–∞—é—Ç—Å—è (at @Rockn_RollBar in...\n",
      "33                                @_Metreveli_ –∏–¥–∏ –Ω–∞—Ö—É–π\n",
      "212    Kanda's fav nasi minyak ayam masak merah üôÜüèªÌ†Ω...\n",
      "21                        @USNATO http://t.co/g7MxA5hEBm\n",
      "134    @Jelenatorrrr_1 –∞—Ö–∞—Ö—Ö–∞—Ö–∞, –µ—Å–ª–∏ —è —Å–ª–∏—à–∫–æ–º –∑–∞—Ö–æ—á...\n",
      "67     –≥–ª–∞–∑–∞ —Å–∞–º–∏ –≥–æ–≤–æ—Ä—è—Ç –∏ –≤—ã—Ä–∞–∂–∞—é—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ) @ Ku...\n",
      "71                              @genrih_m –¥–∞ –∫–∞–Ω–µ–µ–µ—à–Ω–æ–æ–æ\n",
      "60                   @ViktoriaPankova –¥–∞ –∫–ª–∞—Å—Å–Ω–∞—è –±—ã–ª–∞))\n",
      "1      –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—É—Å—Ç—å —É—Ç—Ä–æ–º –≤—Å–µ —ç—Ç–æ –æ–∫–∞–∂–µ—Ç—Å—è –¥—É—Ä–Ω—ã...\n",
      "220                      @zhuravleva –∏ –±–∞—Å—Ç–∞ —Ä–∞–π–º—Å–∞!!!!!\n",
      "78                              –¢—ã –æ–¥–∏–Ω,—Ç—ã —Å–∞–º –∑–∞ —Å–µ–±—è‚õîÔ∏è\n",
      "3      –°–µ–π—á–∞—Å +9¬∞, —è—Å–Ω–æ. –£—Ç—Ä–æ–º +7¬∞, –æ–±–ª–∞—á–Ω–æ, –Ω–µ–±–æ–ª—å—à–æ...\n",
      "107                    I`ll be the beauty queen in tears\n",
      "81     @Screwball_SBMF can't find anything that you m...\n",
      "121    –í –¥–æ–º–µ –Ω–∞–ø—Ä–æ—Ç–∏–≤ –∑–∞–∂–≥–ª–æ—Å—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–∫–æ–Ω. –ü—Ä–∏—Ç–≤...\n",
      "229    –ø–µ—Ä–µ–¥ —Ñ–æ—Ç–æ —Å–µ—Å—Å–∏–µ–π!–°–ø–∞—Å–∏–±–æ #–∞—Ä–º–∞–Ω–∞—Å–ª–∞–Ω—è–Ω –∑–∞ —Ç–æ...\n",
      "131                          Great goal by @ovi8 #gocaps\n",
      "80                                    @zaraiskyi —Å–ø–∞—Å–∏–±–æ\n",
      "94     #–ø—Ä–∏–≥–ª–∞—à–∞—é –≤—Å–µ—Ö –Ω–∞ —Å–≤–æ–∏ #–∫—É—Ä—Å—ã !!! üòÇ #kamma #...\n",
      "116                         @NATO http://t.co/L5L2RaO5DB\n",
      "24                                         –Ω–∏—á–µ–≥–æ–æ–æ —Å–µ–±–µ\n",
      "25                                http://t.co/Vw4q2CffHa\n",
      "199            @CANADANATO LATTER http://t.co/mxkKeejoua\n",
      "14     –Ø –∂–∏–≤—É –≤ –¥–µ—Ä—å–º–µ. –Ø - —Å–µ—Ä–∂–∞–Ω—Ç –ø–æ–ª–∏—Ü–∏–∏ –ù–ô –≤ 80—ã—Ö...\n",
      "59       @ViktoriaPankova –ø–æ-–º–æ–µ–º—É –∏–º–µ–Ω–Ω–æ —Ç–∞–∫–ø—è –∏ –±—ã–ª–∞ !\n",
      "206    @David_Oakes With girls no problem...If you fo...\n",
      "150      @Jelenatorrrr_1 –¥–∞-–¥–∞! \\n–Ø –∑–µ–≤–∞—Ç—å —É–∂–µ –Ω–∞—á–∞–ª —á–µ—Ç\n",
      "                             ...                        \n",
      "77     @adamlambert give us some information about yo...\n",
      "63                                http://t.co/Utvnmcr6I3\n",
      "245                                   @RickyPDillon calm\n",
      "11     –£ –Ω–∞—Å –¥–æ–º–∞ —Å–∞–±–∞–Ω—Ç—É–π üéâüéâüéâ–º–æ–∏ –ª—É—á—à–∏–µ –ø—Ä–∏–µ—Ö–∞–ª–∏ ...\n",
      "142                                    @bubulaaah –æ –±–æ–≥–∏\n",
      "114    #–º–æ–π–º—É–∂ #3:15 #–º–∞–≥–∞–∑–∏–Ω #—Ö–æ—á—É–º–æ—Ä–æ–∂–µ–Ω–∫—É #–ª—é–±–æ–≤—å—Å...\n",
      "10                          @NATO http://t.co/ChybvVrlDW\n",
      "70     –î–æ–º–æ–æ–æ–π (@ –ú–∞–≥–Ω–æ–ª–∏—è in –ú–æ—Å–∫–≤–∞) https://t.co/fT...\n",
      "119    @David_Oakes Yes, you're right. He really does...\n",
      "164    @Jelenatorrrr_1 –ø–µ—Ä–µ—Å—Ç–∞–Ω—å –¥—É—Ç—å—Å—è \\n–≠—Ç–æ —Ä–∞–¥–∏ —Ç–µ...\n",
      "246    @David_Oakes Sorry I can't participate in the ...\n",
      "100               @Jelenatorrrr –≤–º–µ—Å—Ç–µ –ø–æ–π–¥–µ–º —á–µ—Ä–µ–∑ —á–∞—Å?\n",
      "235    –ú—ã –ø–µ—Ä–µ–±—Ä–∞–ª–∏—Å—å –≤ #hiddenbar @ Hidden bar https...\n",
      "98                    @CANADANATO http://t.co/JzsXHA8DCf\n",
      "48                          @NATO http://t.co/1WOOGMt3Ej\n",
      "168                     @TerentKaterina –ö–ê–ö–û–ô –¢–£–¢ –ú–ò–ú–ò–ú–ò\n",
      "201            @CANADANATO LATTER http://t.co/qhW4d9UAUB\n",
      "113                      –°–Ω–æ–≤–∞ –±–µ—Å—Å–æ–Ω–Ω–∞—è –Ω–æ—á—å :) #gocaps\n",
      "88                        @USNATO http://t.co/V485eJBSlU\n",
      "200    –° –¥–≤—É—Ö –¥–æ —á–µ—Ç—ã—Ä–µ—Ö —á–∞—Å–æ–≤ –Ω–æ—á–∏ \\n–ö–æ–Ω–µ—á–Ω–æ, —Å–∞–º–æ–µ ...\n",
      "167    #Gipsy - Grand Opening! 2015 | by #BlazeTV x d...\n",
      "217    –Ø, –≤–∏–¥–∏–º–æ, –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π —á–µ–ª–æ–≤–µ–∫, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—á–µ—Ç...\n",
      "74     –ù–∞—Å—Ç—É–ø–∏–ª–∞ –≤–µ—Å–Ω–∞, –∞ —ç—Ç–æ –∑–Ω–∞—á–∏—Ç: –¥–∞ –∑–¥—Ä–∞–≤—Å—Ç–≤—É–µ—Ç ...\n",
      "148                  @NATO LATTER http://t.co/0bkvxwf482\n",
      "83                        @UKNATO http://t.co/DDR5t9qW4K\n",
      "192                @USNATO LATTER http://t.co/KiNGuuXNvG\n",
      "47     @ViktoriaPankova —Å —Ç–∞–∫–æ–π –í–∏–∫–æ–π —è –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∞—Å—å üòä\n",
      "75                            \"I wanna be with Elena\" üòç\n",
      "57                   @NATO LATTER http://t.co/qoTvsF5dNJ\n",
      "230    @bathbombfranta HOLLLLEEEEEEE KEHSJDKSKS OMG O...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#before\n",
    "inidiciesOfTweets = findSimilarTweets(u'–≤ –º–µ—Ç—Ä–æ –æ–±—ä—è–≤–ª—è—é—Ç —Å—Ç–∞–Ω—Ü–∏–∏',data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Length:  15268\n"
     ]
    }
   ],
   "source": [
    "print 'Vocab Length: ', len(interestingVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–†–µ–±—è—Ç–∞, –º—ã –≤—Å–µ–º —Ä–∞–¥—ã!\n"
     ]
    }
   ],
   "source": [
    "print data['cleanText'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673685"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainigList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f4577b5aa7aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m lsh = LSH( 100,\n\u001b[0;32m----> 6\u001b[0;31m            \u001b[0mtfidf_matrix_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m            \u001b[0mnum_hashtables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m            storage_config={\"dict\":None})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfidf_matrix_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sparselsh import LSH\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "t0 = time()\n",
    "lsh = LSH( 100,\n",
    "           tfidf_matrix_train.shape[1],\n",
    "           num_hashtables=1,\n",
    "           storage_config={\"dict\":None})\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.470348%"
     ]
    }
   ],
   "source": [
    "for ix in xrange(tfidf_matrix_train.shape[0]):\n",
    "    x = tfidf_matrix_train.getrow(ix)\n",
    "    lsh.index(x)\n",
    "    progress(ix,tfidf_matrix_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nearpy import Engine\n",
    "from nearpy.hashes import RandomBinaryProjections\n",
    "\n",
    "# Dimension of our vector space\n",
    "dimension = 478524\n",
    "\n",
    "# Create a random binary hash with 10 bits\n",
    "rbp = RandomBinaryProjections('rbp', 1000)\n",
    "\n",
    "# Create engine with pipeline configuration\n",
    "engine = Engine(dimension, lshashes=[rbp])\n",
    "\n",
    "# Index 1000000 random vectors (set their data to a unique string)\n",
    "for index in range(673685):\n",
    "    v = tfidf_matrix_train[index:index+1]\n",
    "    engine.store_vector(v, 'data_%d' % index)\n",
    "    progress(index,673685)\n",
    "\n",
    "# Create random query vector\n",
    "query = numpy.random.randn(dimension)\n",
    "\n",
    "# Get nearest neighbours\n",
    "N = engine.neighbours(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
