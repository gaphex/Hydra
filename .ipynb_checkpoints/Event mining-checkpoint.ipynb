{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import time\n",
    "import tweepy\n",
    "import random\n",
    "import urllib2\n",
    "import Stemmer\n",
    "import sqlite3\n",
    "import operator\n",
    "import pymorphy2\n",
    "%matplotlib inline\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import stdout\n",
    "from lshash import LSHash\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Hyperion():\n",
    "    def __init__(self, data):\n",
    "        print 'Invoking Hyperion...'\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.stemmer = Stemmer.Stemmer('russian')\n",
    "        self.data = data\n",
    "        \n",
    "    def processContents(self, myText):\n",
    "        myText = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))', '', myText)\n",
    "        words = [word for word in re.findall(r'(?u)[@|#]?\\w+', myText) if not word.startswith(('@','#'))]\n",
    "        words = self.stemmer.stemWords(words)\n",
    "        return words\n",
    "        \n",
    "    def preprocess(self):\n",
    "        \n",
    "        print 'Preprocessing...this may take a while'\n",
    "        t0 = time.time()\n",
    "        terms = []\n",
    "        words = []\n",
    "\n",
    "        n = len(data.index)\n",
    "\n",
    "        for i in range(n):\n",
    "            terms.append(self.processContents(self.data.content_lower[i]))\n",
    "\n",
    "        self.data['terms'] = terms[:]\n",
    "\n",
    "        for i in range(n):\n",
    "            words += terms[i]\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        sortedDist = [x for x in sortedDist if len(x[0]) > 2]\n",
    "        interestingVocab = [x[0] for x in sortedDist]\n",
    "\n",
    "        #Find TF-IDF\n",
    "\n",
    "        trainingList = []\n",
    "        for i in range(n):\n",
    "            trainingList.append(' '.join(data['terms'][i]))\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(vocabulary = interestingVocab)\n",
    "        self.tfidf = self.vectorizer.fit_transform(trainingList)  #finds the tfidf score with normalization\n",
    "\n",
    "        print 'Building vocab and TF-IDF matrix took', time.time()-t0, 'seconds'\n",
    "        print 'Vocab Length =', len(interestingVocab), '    Vector dimensionality =', n\n",
    "        return(self.vectorizer, self.tfidf)\n",
    "\n",
    "\n",
    "    def progress(self, i, n):\n",
    "        stdout.write(\"\\r%f%%\" % (i*100/float(n)))\n",
    "        stdout.flush()\n",
    "        if i == n-1:\n",
    "            stdout.write(\"\\r100%\")\n",
    "            print(\"\\r\\n\")\n",
    "        \n",
    "    def median(self,lst):\n",
    "        return np.median(np.array(lst))\n",
    "\n",
    "    def extract_urls(self, lst):\n",
    "        urls = []\n",
    "        for i in lst:\n",
    "            for j in i.split(' '):\n",
    "                if j.startswith('http'):\n",
    "                    urls.append(j)\n",
    "        return urls\n",
    "\n",
    "    def resolve_url(self, starturl):\n",
    "        try:\n",
    "            req = urllib2.Request(starturl)\n",
    "            res = urllib2.urlopen(req, timeout = 2)\n",
    "            finalurl = res.geturl()\n",
    "            return finalurl\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def setup(self, vectorizer, tf_idf):\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tfidf = tf_idf\n",
    "        print 'Setup complete '\n",
    "        \n",
    "\n",
    "    def findSimilarTweets(self, queryTweet, threshold, maxNumber = 0, log = False):\n",
    "\n",
    "        t0 = time.time()\n",
    "        \n",
    "        processedTweet = ' '.join(self.processContents(queryTweet))\n",
    "        queryTweetRepresentation = self.vectorizer.transform([processedTweet])\n",
    "\n",
    "        cosine_similarities = cosine_similarity(queryTweetRepresentation, self.tfidf)[0]\n",
    "        totalMatchingTweets = len(cosine_similarities[cosine_similarities>threshold])\n",
    "\n",
    "        if maxNumber:\n",
    "            totalMatchingTweets = min(totalMatchingTweets, maxNumber)\n",
    "        indices = cosine_similarities.argsort()[::-1][:totalMatchingTweets]\n",
    "        elapsed_time = time.time() - t0\n",
    "        if len(indices) > 25:\n",
    "            print 'Query:', queryTweet\n",
    "        return indices\n",
    "\n",
    "    def findOut(self, cluster):\n",
    "        lst = list(self.data['content'][cluster])\n",
    "\n",
    "        redirects = []\n",
    "        keywords = []\n",
    "\n",
    "        urls = self.extract_urls(lst)\n",
    "        for url in urls:\n",
    "            redirects.append(self.resolve_url(url))\n",
    "\n",
    "        corpus = ' '.join(lst).lower()\n",
    "        corpus = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))', '', corpus)\n",
    "        words = []\n",
    "        tokenized = nltk.tokenize.word_tokenize(corpus)\n",
    "        for word in tokenized:\n",
    "            words.append(word)\n",
    "\n",
    "        fdist = nltk.FreqDist(words)\n",
    "        sortedDist = sorted(fdist.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        moreThan3 = [x for x in sortedDist if len(x[0]) > 3 and x[1] > 3]\n",
    "        for i in moreThan3:\n",
    "            keywords.append(i[0])\n",
    "\n",
    "        return(keywords, redirects)\n",
    "\n",
    "    def resolve(self, masterCluster):\n",
    "        t0 = time.time()\n",
    "        k, r = self.findOut(masterCluster)\n",
    "\n",
    "        insta = []\n",
    "        swarm = []\n",
    "\n",
    "        print 'Found', len(r), 'anonymous links, investigating...'\n",
    "        for i, u in enumerate(r):\n",
    "            if u:\n",
    "                if 'instagram' in u:\n",
    "                    insta.append(u)\n",
    "                if 'swarmapp' in u:\n",
    "                    swarm.append(u)\n",
    "\n",
    "        print 'Resolving took', time.time()-t0\n",
    "        return ((k, insta, swarm))\n",
    "\n",
    "    def overlap(self, r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b):\n",
    "        #Overlapping rectangles overlap both horizontally & vertically\n",
    "        return self.range_overlap(r1l, r1r, r2l, r2r) and self.range_overlap(r1b, r1t, r2b, r2t)\n",
    "\n",
    "    def range_overlap(self, a_min, a_max, b_min, b_max):\n",
    "        #Neither range is completely greater than the other\n",
    "        return (a_min <= b_max) and (b_min <= a_max)\n",
    "\n",
    "    def assess(self, clusters):\n",
    "        data = []\n",
    "        for c in clusters:\n",
    "\n",
    "            xs = [t[0] for t in c]\n",
    "            ys = [t[1] for t in c]\n",
    "            z = zip(xs, ys)\n",
    "            n = len(z)\n",
    "\n",
    "            dx = max(xs) - min(xs)\n",
    "            dy = max(ys) - min(ys)\n",
    "\n",
    "            mx = self.median(xs)\n",
    "            my = self.median(ys)\n",
    "\n",
    "            s=(dx+0.001)*(dy+0.001)\n",
    "            score = n/s\n",
    "\n",
    "            data.append((z,n,score,dx,dy,mx,my))\n",
    "            data = filter(lambda a: a[1] > 5, data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def findUnique(self, stats):\n",
    "\n",
    "        for v in range(3):\n",
    "            for i in stats:\n",
    "                r1l = i[5]-2*i[3]\n",
    "                r1r = i[5]+2*i[3]\n",
    "                r1b = i[6]-2*i[4]\n",
    "                r1t = i[6]+2*i[4]\n",
    "                for j in stats:\n",
    "                    r2l = j[5]-2*j[3]\n",
    "                    r2r = j[5]+2*j[3]\n",
    "                    r2b = j[6]-2*j[4]\n",
    "                    r2t = j[6]+2*j[4]\n",
    "                    if self.overlap(r1l, r1r, r1t, r1b, r2l, r2r, r2t, r2b) and i!=j:\n",
    "                        if i[1] >= j[1]:\n",
    "                            try:\n",
    "                                stats.remove(j)\n",
    "                            except:\n",
    "                                pass\n",
    "                        else:\n",
    "                            try:\n",
    "                                stats.remove(i)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    def performClusterisation(self, indices, thSp, thTm):\n",
    "    \n",
    "        #Spatial Clustering\n",
    "        y = list(data['lat'][indices])\n",
    "        x = list(data['long'][indices])\n",
    "        tid = indices\n",
    "\n",
    "        z = filter(lambda a: a[0] != (55.753301, 37.619899), zip(zip(x,y), tid))\n",
    "        x = []\n",
    "        y = []\n",
    "        tid = []\n",
    "\n",
    "        for t in z:\n",
    "            x.append(t[0][0])\n",
    "            y.append(t[0][1])\n",
    "            tid.append(t[1])\n",
    "\n",
    "        crds = zip(x,y)\n",
    "        spUniques = self.findCluster(crds, tid, 34, thSp, thTm, 'space')\n",
    "        if len(spUniques):\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', len(spUniques), 'spatial clusters with a total of', sum([len(t[0]) for t in spUniques]), 'points among', len(indices)\n",
    "\n",
    "        for w in spUniques:\n",
    "            print len(w[0]), 'points at', w[3], w[4]\n",
    "\n",
    "\n",
    "        #Temporal Clustering\n",
    "        tmvc = []\n",
    "        for t in data['created_at'][indices]:\n",
    "            l = t.split(' ')\n",
    "            date = l[0].split('-')\n",
    "            time = l[1].split(':')\n",
    "            datehash = int(date[1]) * 30 + int(date[2])\n",
    "            timehash = int(time[0]) * 3600 + int(time[1]) * 60 + int(time[2])\n",
    "            tmvc.append((datehash*400, timehash))\n",
    "\n",
    "        tpUniques = self.findCluster(tmvc, tid, 34, thSp, thTm, 'time')\n",
    "        dm = len(tpUniques)\n",
    "\n",
    "        if dm:\n",
    "            print '--------------------------------------------------------------'\n",
    "            print 'Found', dm, 'temporal clusters with a total of', sum([len(t[0]) for t in tpUniques]), 'points among', len(indices)\n",
    "        \n",
    "\n",
    "        for w in tpUniques:\n",
    "            date = w[3]/400\n",
    "            time = w[4]\n",
    "            print len(w[0]), 'points at', int(date//30),'.',int(date%30),'--',int(time//3600),\n",
    "            print ':',int(time%3600//60),':',int(time%3600%60)\n",
    "\n",
    "        print ''\n",
    "        return ((spUniques, tpUniques))\n",
    "\n",
    "    def findCluster(self, vecs, tids, cln, thSp, thTm, mode):\n",
    "\n",
    "        maxn = 0\n",
    "        maxc = []\n",
    "        dim = len(vecs[0])\n",
    "        data = zip(vecs, tids)\n",
    "\n",
    "\n",
    "        if mode == 'space':\n",
    "            metric = \"euclidean\"\n",
    "            th = thSp\n",
    "            rat = 1\n",
    "        if mode == 'time':\n",
    "            metric = 'hamming'\n",
    "            th = thTm\n",
    "            rat = 1\n",
    "\n",
    "        lsh = LSHash(cln, dim)\n",
    "        for i in vecs:\n",
    "            lsh.index(i)\n",
    "\n",
    "        pivots = list(set(random.sample(vecs, int(rat*len(vecs)))))\n",
    "\n",
    "        for i in pivots:\n",
    "            mxc = []\n",
    "            dists = []\n",
    "            u = lsh.query(i, distance_func=metric)\n",
    "\n",
    "            for j in u:\n",
    "                dists.append(j[1])\n",
    "                mxc.append(j[0])\n",
    "\n",
    "            r = self.cover(dists, th)\n",
    "            maxc.append(mxc[0:r])\n",
    "\n",
    "\n",
    "        scr = self.assess(maxc)\n",
    "        rez = self.findUnique(scr)\n",
    "\n",
    "        clusters = []\n",
    "        for u in rez:\n",
    "            cIDs = []\n",
    "            for t in u[0]:\n",
    "                for d in data:\n",
    "                    if t == d[0]:\n",
    "                        cIDs.append(d[1])\n",
    "            clusters.append((cIDs, u[3], u[4], u[5], u[6]))\n",
    "\n",
    "        clusters = filter(lambda a: len(a[0]) > 6, clusters)\n",
    "\n",
    "        return (clusters)\n",
    "\n",
    "    def cover(self, dat, th):\n",
    "        sum = 0\n",
    "        t = 0\n",
    "        while sum < th*len(dat) and t < 0.5*len(dat):\n",
    "            sum += dat[t]\n",
    "            t += 1\n",
    "        return t\n",
    "\n",
    "    def doQuery(self, query, th_NN, th_SP, th_TM):\n",
    "        t0 = time.time()\n",
    "        indices = self.findSimilarTweets(query, th_NN)\n",
    "        print len(indices), 'points passed preprocessing'\n",
    "\n",
    "        if len(indices):\n",
    "            t = self.performClusterisation(indices, th_SP, th_TM)\n",
    "            print \"Query processing took\", time.time()-t0, 'seconds'\n",
    "\n",
    "        return t\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Preprocessing...this may take a while\n",
      "Building vocab and TF-IDF matrix took 155.538730145 seconds\n",
      "Vocab Length = 414550     Vector dimensionality = 1315775\n"
     ]
    }
   ],
   "source": [
    "con = sqlite3.connect('tweetsSpring.db')\n",
    "data = pd.read_sql(\"SELECT * from tweets\",con) #WHERE content LIKE '%–≥—Ä–æ–∑–∞%' COLLATE NOCAS\n",
    "\n",
    "hyperion = Hyperion(data)\n",
    "vec, tf = hyperion.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: —è –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–µ\n",
      "355 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 39 points among 355\n",
      "19 points at 55.7553515 37.618101\n",
      "9 points at 55.782217 37.716641\n",
      "11 points at 55.8512005 37.4431215\n",
      "--------------------------------------------------------------\n",
      "Found 1 temporal clusters with a total of 7 points among 355\n",
      "7 points at 4 . 15 -- 16 : 48 : 22\n",
      "\n",
      "Query processing took 1.76247406006 seconds\n"
     ]
    }
   ],
   "source": [
    "u = hyperion.doQuery(u'—è –≤ –∫–∏–Ω–æ—Ç–µ–∞—Ç—Ä–µ', 0.34, 0.000005, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoking Hyperion...\n",
      "Setup complete \n"
     ]
    }
   ],
   "source": [
    "hyperion = Hyperion(data)\n",
    "hyperion.setup(vec, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "media = []\n",
    "for clusterType in u:\n",
    "    for cluster in clusterType:\n",
    "        z = hyperion.resolve(cluster[0])\n",
    "        print media\n",
    "        media.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for photo in media[0][1]:\n",
    "    print photo[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "CONSUMER_KEY = ['KpfGPpsl5Dn03Lb5wzvQfEaMc',\n",
    "                '13AqFSrFdFv7rdLVOGvzJCkmp',\n",
    "                '45RuEYLg5eVTYyEGuyEerplyY',\n",
    "                '3qTMdAYKxctRe69HMDqyeNST2',\n",
    "                'JJkrFkKGlhDIEgj2eTrQ']\n",
    "\n",
    "CONSUMER_SECRET = ['UWRvjR3CHsducO1i7268F24C3M9UJu5U7p2u4kh6Ds6QMDdKCg',\n",
    "                   'LVaOSMsMBWl4FmgthjNPWMnkKe7MXKXrmu5uL6JnJWIhHieDxR',\n",
    "                   'LBnbBTIAhtYYBU6RWeyCzgIcJannob7bPrzg3dMqFuRDLJnbHp',\n",
    "                   'Jwwv3wHzL2jtYMHylakpjmDxf5SgvAwexFGfEoCFHw92f65lnK',\n",
    "                   'H7hmUQXqXseKbj1WnKFMnaURyQbBaDeyK3DAAwLI']\n",
    "\n",
    "OAUTH_TOKEN = ['2181757628-o8IOmHBelyhVM6KEkkT50ZLIbv4fj6llW6KSjpd',\n",
    "               '175663996-ZNL1MivJASYSxWsNXlxNHnQhmLHDegH9VdVfATsL',\n",
    "               '175663996-lQRf1JNjvR1fVILTtoEH4FHVQ1sLtPa0IIa8lMog',\n",
    "               '2841198550-rlPUcMyCj8rk3Yv6XxGJWk0ELCCUGUrxhvYyAa6',\n",
    "               '2181757628-0n3FpGEtoob0qum7IMeN3R0oV1kg5STZwmXNa9Q']\n",
    "\n",
    "OAUTH_TOKEN_SECRET = ['cyLlZtQyv4rgcWA5pGaXLtGJaFqD4PGOlxSdb4ECVzoSP',\n",
    "                      'gJOHvvlcObkiu7Qd91WapTFwnOVsisdoeMBUHxcFfzBac',\n",
    "                      'eQ0SUwziSUgRs72HJzWpU9IAlVP92X9YJsGHOPrWUctw3',\n",
    "                      '9b36g1wXLzn1yB0FGIoT9eACxPPpaZVfESnmRYcDYk3wv',\n",
    "                      'MqgrZHb8CMyNqFJn36YmCtLUQ5rqNUzX2IxWNfQdHQ6t7']\n",
    "\n",
    "auths = []\n",
    "\n",
    "up = 55.96\n",
    "down = 55.49\n",
    "right = 37.97\n",
    "left = 37.32\n",
    "\n",
    "for i in range(5):\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY[i], CONSUMER_SECRET[i])\n",
    "    auth.set_access_token(OAUTH_TOKEN[i], OAUTH_TOKEN_SECRET[i])\n",
    "    auths.append(auth)\n",
    "\n",
    "\n",
    "\n",
    "class CustomStreamListener(tweepy.StreamListener):\n",
    "    \n",
    "    def on_status(self, status):\n",
    "        try:\n",
    "            tid = status.id_str\n",
    "            usr = status.author.screen_name.strip()\n",
    "            txt = status.text.strip()\n",
    "\n",
    "            indices = hyperion.findSimilarTweets(txt, 0.5)\n",
    "            \n",
    "            if len(indices) > 34:\n",
    "                try:\n",
    "                    hyperion.doQuery(txt, 0.34, 0.000005, 5000)\n",
    "                except:\n",
    "                    print 'Caught exception during clusterization'\n",
    "    \n",
    " \n",
    "        except Exception as e:\n",
    "            # Most errors we're going to see relate to the handling of UTF-8 messages (sorry)\n",
    "            print('BULLSHIT', e)\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print >> sys.stderr, 'Encountered error with status code:', status_code\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print >> sys.stderr, 'Timeout...'\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "    def on_disconnect(self, notice):\n",
    "        print notice\n",
    "        return True # Don't kill the stream\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: @morozovbest –ü–æ—Ä–æ—à–µ–Ω–∫–æ –Ω–µ–ø—Ä–µ–º–µ–Ω–Ω–æ –Ω–∞–≤–µ—Å—Ç–∏—Ç –ø–µ—Ä–µ–ª–æ–º–∞–Ω–Ω–æ–≥–æ –ö–µ—Ä—Ä–∏ –∏ –ø–æ–¥–∞—Ä–∏—Ç –µ–º—É —Ñ—É—Ç–±–æ–ª—å–Ω—ã–π –º—è—á\n",
      "Query: üîù http://t.co/F41nTHUv4N\n",
      "Query: –∫–∞–∫–∏–µ –æ–Ω–∏ –æ—á–∞—Ä–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ üòª http://t.co/ZsKcGtEgKG\n",
      "Query: –∫–∞–∫–∏–µ –æ–Ω–∏ –æ—á–∞—Ä–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ üòª http://t.co/ZsKcGtEgKG\n",
      "1178 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 7 spatial clusters with a total of 88 points among 1178\n",
      "21 points at 55.757911 37.609958\n",
      "11 points at 55.896612 37.398674\n",
      "7 points at 55.606148 37.597804\n",
      "8 points at 55.730543 37.816314\n",
      "9 points at 55.929383 37.844581\n",
      "14 points at 55.7370125 37.474013\n",
      "18 points at 55.8931545 37.6215015\n",
      "--------------------------------------------------------------\n",
      "Found 1 temporal clusters with a total of 7 points among 1178\n",
      "7 points at 4 . 14 -- 12 : 18 : 17\n",
      "\n",
      "Query processing took 7.7037229538 seconds\n",
      "Query: –°–æ–±—Ä–∞–ª–∏—Å—å –≤—Å–µ–º –∫–ª–∞—Å—Å–æ–º –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è üòÑ\n",
      "–õ—é–±–∏–º—ã–µ üòö‚ù§Ô∏è @ –°—Ç–æ–ª–µ—à–Ω–∏–∫–æ–≤ –ø–µ—Ä–µ—É–ª–æ–∫ https://t.co/ez6WnX4CB4\n",
      "Query: –≠—Ç–æ –ø—Ä–æ—Å—Ç–æ –Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ –Ω–µ –ø—Ä–æ–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å!\n",
      "Query: –ñ–∏–∑–Ω—å –≤–∏—Å–∏—Ç –Ω–∞ –Ω–∏—Ç–∫–µ,–∞ –¥—É–º–∞—é—Ç –≤—Å–µ –æ –ø—Ä–∏–±—ã—Ç–∫–µ?!! #–∞–ø–æ–∫–∞–ª–∏–ø—Å–∏—Å2016 #–∫—Ä–∞—Ö–°–®–ê #–º–∏—Ñ–æ–ª–æ–≥–∏—è #US #–Ω–∞–µ–º–Ω–∏–∫–∏ #stopTerrorUSA  https://t.co/TjIHcAnyDp\n",
      "Query: –∏ —è –≤—Å–µ –µ—â–µ –Ω–µ –º–æ–≥—É –æ—Ç–æ–π—Ç–∏ –æ—Ç —Ç–æ–≥–æ –≤–∞–π–Ω–∞ \n",
      "–≥—Å–ø–¥\n",
      "Query: @NATO http://t.co/iuiGmeQFdL\n",
      "Query: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ–ø–∞—Å–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –∑–∞ —Å–µ–≥–æ–¥–Ω—è\n",
      "Query: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –æ–ø–∞—Å–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –∑–∞ —Å–µ–≥–æ–¥–Ω—è\n",
      "434 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 15 points among 434\n",
      "7 points at 55.622455 37.490585\n",
      "8 points at 55.7636135 37.615162\n",
      "\n",
      "Query processing took 1.79436182976 seconds\n",
      "Query: –ß–µ—Ä–µ–∑ –ø–æ–ª—á–∞—Å–∞ –ª–µ—Ç–æ!!\n",
      "Query: –ß–µ—Ä–µ–∑ –ø–æ–ª—á–∞—Å–∞ –ª–µ—Ç–æ!!\n",
      "389 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 15 points among 389\n",
      "7 points at 55.756574 37.656901\n",
      "8 points at 55.8063395 37.498773\n",
      "\n",
      "Query processing took 1.62681102753 seconds\n",
      "Query: @chiuuri —Å–ª–∞–¥–∫–∏—Ö —Å–Ω–æ–≤ :–∑\n",
      "Query: @chiuuri —Å–ª–∞–¥–∫–∏—Ö —Å–Ω–æ–≤ :–∑\n",
      "2061 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 5 spatial clusters with a total of 146 points among 2061\n",
      "61 points at 55.891143 37.7203465\n",
      "12 points at 55.980498 37.1561835\n",
      "19 points at 55.622485 37.490667\n",
      "22 points at 55.6795015 37.851008\n",
      "32 points at 55.817334 37.499944\n",
      "--------------------------------------------------------------\n",
      "Found 5 temporal clusters with a total of 55 points among 2061\n",
      "11 points at 4 . 1 -- 21 : 53 : 6\n",
      "9 points at 4 . 18 -- 23 : 37 : 53\n",
      "15 points at 4 . 23 -- 20 : 44 : 48\n",
      "12 points at 5 . 19 -- 20 : 3 : 29\n",
      "8 points at 5 . 23 -- 23 : 17 : 11\n",
      "\n",
      "Query processing took 34.7458641529 seconds\n",
      "Query: –ì–æ—Å—É–¥–∞—Ä–µ–≤–∞ –î—É–º–∞ –∏ –º–µ—Ç—Ä–æ) #moscow #russia #vscocam @ –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –î—É–º–∞ –§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–≥–æ –°–æ–±—Ä–∞–Ω–∏—è‚Ä¶ https://t.co/1NkvikLYJe\n",
      "Query: –ì–æ—Å—É–¥–∞—Ä–µ–≤–∞ –î—É–º–∞ –∏ –º–µ—Ç—Ä–æ) #moscow #russia #vscocam @ –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –î—É–º–∞ –§–µ–¥–µ—Ä–∞–ª—å–Ω–æ–≥–æ –°–æ–±—Ä–∞–Ω–∏—è‚Ä¶ https://t.co/1NkvikLYJe\n",
      "151 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 118 points among 151\n",
      "118 points at 55.758048 37.616049\n",
      "\n",
      "Query processing took 0.914762973785 seconds\n",
      "Query: #–ò—Ä–∞–∫. –î–∞–Ω–Ω—ã–π –≤–∏–¥ —Ñ–ª–∞–≥–∞ –±—ã–ª —É—Ç–≤–µ—Ä–∂–¥–µ–Ω –≤ 2008 –∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø–æ –Ω–∞—Å—Ç–æ—è—â–µ–µ –≤—Ä–µ–º—è. –° 1920–≥. –û–Ω –º–µ–Ω—è–ª—Å—è 6‚Ä¶ https://t.co/Qg2cLsDmhm\n",
      "Query: @Series_Fun98 –¥–∞, –≤ –≥—Ä—É–ø–ø–µ –ø–æ –±—Ä–∏—Ç—Ç —Å–Ω–æ—É\n",
      "Query: –õ–µ–∂—É –Ω–∞ –¥–∏–≤–∞–Ω–µ, —Å–º–æ—Ç—Ä—é –∫–∞–∫ –ø–∞–ø–∞ –∏–≥—Ä–∞–µ—Ç –≤ FarCry –∏ –≤–æ–æ–±—â–µ –≤—Å–µ –æ—Ñ–∏–≥–µ–Ω–Ω–æ\n",
      "Query: http://t.co/gM4b9M2IfE\n",
      "Query: –ò –º—ã –ø–µ—Ä–µ—à–ª–∏ –Ω–∞ –Ω–æ–≤—É—é —Å—Ç—É–ø–µ–Ω—å –Ω–∞—à–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π. –ò —Å–≤–∞–ª–∏–ª–∏—Å—å —Å –Ω–µ–µ, —Ç.–∫. —É —Ñ–ª—ç—á–µ—Ä–∞ –≤ –¥—É—à–µ –∑–∞–∏–≥—Ä–∞–ª–∞ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∫–∞\n",
      "Query: @viktessa –∏ —á–∞—Å–∞–º–∏. –ù–µ.\n",
      "Query: @viktessa –∏ —á–∞—Å–∞–º–∏. –ù–µ.\n",
      "2317 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 5 spatial clusters with a total of 128 points among 2317\n",
      "9 points at 55.994064 37.185103\n",
      "14 points at 55.90129 37.40154\n",
      "50 points at 55.599456 37.669286\n",
      "17 points at 55.680597 37.858167\n",
      "38 points at 55.7599025 37.620751\n",
      "--------------------------------------------------------------\n",
      "Found 5 temporal clusters with a total of 40 points among 2317\n",
      "7 points at 4 . 16 -- 9 : 20 : 18\n",
      "11 points at 4 . 24 -- 17 : 13 : 33\n",
      "7 points at 4 . 0 -- 20 : 32 : 35\n",
      "8 points at 5 . 19 -- 18 : 35 : 41\n",
      "7 points at 5 . 13 -- 14 : 5 : 9\n",
      "\n",
      "Query processing took 30.2573149204 seconds\n",
      "Query: @tema_vanilla –Ω—É –ª–∞–¥–Ω–æ(\n",
      "–ò–Ω–æ–≥–¥–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–±–∞–≤–Ω–æ((\n",
      "Query: @tema_vanilla –Ω—É –ª–∞–¥–Ω–æ(\n",
      "–ò–Ω–æ–≥–¥–∞ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–±–∞–≤–Ω–æ((\n",
      "744 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 31 points among 744\n",
      "8 points at 55.8033385 37.498818\n",
      "7 points at 55.599455 37.669021\n",
      "16 points at 55.8911415 37.7203635\n",
      "\n",
      "Query processing took 3.13643598557 seconds\n",
      "Query: –°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏‚≠êÔ∏èüåü‚ú®üí´\n",
      "Query: –°–ø–æ–∫–æ–π–Ω–æ–π –Ω–æ—á–∏‚≠êÔ∏èüåü‚ú®üí´\n",
      "2979 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 8 spatial clusters with a total of 205 points among 2979\n",
      "30 points at 55.681949 37.848151\n",
      "15 points at 55.3256455 37.8123545\n",
      "32 points at 55.664957 37.498989\n",
      "12 points at 55.501481 38.364402\n",
      "30 points at 55.8766715 37.688026\n",
      "20 points at 56.038302 37.245166\n",
      "25 points at 55.600836 37.653428\n",
      "41 points at 55.8063705 37.498802\n",
      "--------------------------------------------------------------\n",
      "Found 4 temporal clusters with a total of 56 points among 2979\n",
      "15 points at 4 . 23 -- 20 : 25 : 2\n",
      "17 points at 5 . 13 -- 21 : 4 : 33\n",
      "17 points at 4 . 1 -- 20 : 28 : 7\n",
      "7 points at 5 . 23 -- 23 : 46 : 23\n",
      "\n",
      "Query processing took 67.3033671379 seconds\n",
      "Query: #@Poroshenko,  #Poroshenko, —ç–π –ø–æ—Ç–æ–º–æ–∫ –í–∏—è –Ω–∞ –ø–ª–µ—á–µ —É —Ç–µ–±—è –∏ –Ω–µ –æ–¥–∏–Ω, –Ω–µ –¥–∞–≤–∏—Ç, –Ω–æ–∂–∫–∏ –Ω–µ –±–æ–ª—è—Ç ,–ü–µ—Ç—Ä–æ!!!\n",
      "Query: –ü–æ—Ç–∞–Ω–∏–Ω–∞ —Ç–ø\n",
      "Query: @mariamatildusha @lexusarh2009 –¶–∞–ø–ª–∏–µ–Ω–∫–æ —ç—Ç–æ —Ä–µ–¥–∫–∏–π –ø–æ —Å—É—Ç–∏ –≥–∞–Ω–¥–æ–Ω.–ú—Ä–∞–∑—å\n",
      "Query: –°–ê–ú–´–ï –°–£–ö–ê –ê–•–£–ï–ù–ù–´–ï –†–ï–ë–Ø–¢–ê http://t.co/hT6Hp6vOP8\n",
      "Query: –°–ê–ú–´–ï –°–£–ö–ê –ê–•–£–ï–ù–ù–´–ï –†–ï–ë–Ø–¢–ê http://t.co/hT6Hp6vOP8\n",
      "531 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 4 spatial clusters with a total of 32 points among 531\n",
      "8 points at 55.7683165 37.643781\n",
      "8 points at 55.807954 37.4976705\n",
      "9 points at 55.737825 37.470683\n",
      "7 points at 55.642395 37.521987\n",
      "\n",
      "Query processing took 1.58270192146 seconds\n",
      "Query: @NATO http://t.co/HTICba1ieH\n",
      "Query: @ananaskalee –ø–ª—é—Å –ø–ª—é—Å\n",
      "Query: @ananaskalee –ø–ª—é—Å –ø–ª—é—Å\n",
      "518 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 3 spatial clusters with a total of 23 points among 518\n",
      "7 points at 55.891147 37.720395\n",
      "8 points at 55.798446 37.4001105\n",
      "8 points at 55.763829 37.6352145\n",
      "\n",
      "Query processing took 1.89278292656 seconds\n",
      "Query: @anankafishbeyn9 –°—É–∫, —Ç—ã –ø—Ä—è–º –∫–∞–∫ –∑–Ω–∞–µ—à—å, –≥–¥–µ —É –º–µ–Ω—è –ø–ª–æ—Ö–æ –≤–∞–π—Ñ–∞–π –ª–æ–≤–∏—Ç –∞–∑–∞—Ö–∞–∑–∞–¥\n",
      "Query: @quarter_miyo –∞–≥–∞ http://t.co/rwI0IZo8GL\n",
      "Query: @frankjoy_gw –ø—Ä—è–º –∫–∞–ø–µ–ª—å–∫—É —Å–æ–≤—Å–µ–º\n",
      "Query: –ï–ì–≠ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –£–∫—Ä–∞–∏–Ω—ã. http://t.co/kxXLQ009SL\n",
      "Query: –ï–ì–≠ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –£–∫—Ä–∞–∏–Ω—ã. http://t.co/kxXLQ009SL\n",
      "401 points passed preprocessing\n",
      "\n",
      "Query processing took 1.69189310074 seconds\n",
      "Query: @pashkal –Ω–µ, –Ω—É –¥–∞–∂–µ wi-fi –ø–æ—á—Ç–∏ —Ä–∞–±–æ—Ç–∞–µ—Ç. –©–∞—Å –Ω–∞–≤–µ—Ä–Ω–æ–µ –≤—Å–µ —É—Å–Ω—É—Ç –∏ —Å–æ–≤—Å–µ–º –∑–∞—Ä–∞–±–æ—Ç–∞–µ—Ç.\n",
      "Query: –¢–∞–Ω—Ü—ã, —Ç–∞–Ω—Ü—ã —Ç–∞–Ω—Ü—É–µ—Ç –¥–µ–≤—á–æ–Ω–∫–∞,\n",
      "–ê –ø–∞—Ä–Ω–∏ –ø—É—Å—Ç—å –ø–æ—Å—Ç–æ—è—Ç –≤ —Å—Ç–æ—Ä–æ–Ω–∫–µ.\n",
      "–ò –≤ –∫–∞–∂–¥–æ–π –Ω–æ—Ç–µ –¥–≤–∏–∂–µ–Ω–∏–µ‚Ä¶ https://t.co/BvmUDcAd2k\n",
      "Query: –¢–∞–Ω—Ü—ã, —Ç–∞–Ω—Ü—ã —Ç–∞–Ω—Ü—É–µ—Ç –¥–µ–≤—á–æ–Ω–∫–∞,\n",
      "–ê –ø–∞—Ä–Ω–∏ –ø—É—Å—Ç—å –ø–æ—Å—Ç–æ—è—Ç –≤ —Å—Ç–æ—Ä–æ–Ω–∫–µ.\n",
      "–ò –≤ –∫–∞–∂–¥–æ–π –Ω–æ—Ç–µ –¥–≤–∏–∂–µ–Ω–∏–µ‚Ä¶ https://t.co/BvmUDcAd2k\n",
      "257 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 2 spatial clusters with a total of 21 points among 257\n",
      "14 points at 55.756689 37.614108\n",
      "7 points at 55.807211 37.522708\n",
      "\n",
      "Query processing took 1.05461096764 seconds\n",
      "Query: –º–æ—ë –∏–º—è –≤ —Ç–∞–∫–æ–π –ø–µ—Ä–µ–∫–ª–∞–¥–∫–µ –±—É–¥–µ—Ç –∑–≤—É—á–∞—Ç—å –ø—Ä–∏–º–µ—Ä–Ω–æ –∫–∞–∫ –ê—Ä—å—è–¥ –°–∏–≤–µ–ª–∏–∞—Å—Ç–æ–≤—Ä–µ\n",
      "Query: –î–µ–ª–∞–ª–∞ —Å —É—Ç—Ä–∞ –Ω–µ–º–µ—Ü–∫–∏–π\n",
      "\n",
      "@\n",
      "\n",
      "–í –¥–µ–≤—è—Ç—å –≤–µ—á–µ—Ä–∞ –∑–≤–æ–Ω–∏—Ç –¶–µ–ø–∫–æ–≤ –∏ –æ—Ç–º–µ–Ω—è–µ—Ç –ø–∞—Ä—ã\n",
      "\n",
      "@\n",
      "\n",
      "–ù—É –ø–∞—Å–∏–±–∞\n",
      "Query: @NATO http://t.co/JuUUmAq72J\n",
      "Query: congratulations @arminvanbuuren #arminvanbuuren read Twitter 100 !!! you're the #best #COOL #LIFEFISHKA http://t.co/yvsAoEsLjT\n",
      "Query: @kristin061996 –≤ 00-00 –æ—Ä–µ—Ö –≤ –æ–∫–Ω–æ \"–ª–µ—Ç–æ–æ–æ–æ \" –∞—Ö–∞—Ö–∞—Ö—Ö–∞\n",
      "Query: –æ–π –∫–∞–∫–∞—è –Ω–µ–ª–æ–≤–∫–æ—Å—Ç—å\n",
      "Query: –æ–π –∫–∞–∫–∞—è –Ω–µ–ª–æ–≤–∫–æ—Å—Ç—å\n",
      "1368 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 6 spatial clusters with a total of 76 points among 1368\n",
      "8 points at 55.6017325 37.597667\n",
      "11 points at 55.896612 37.398674\n",
      "12 points at 55.7344075 37.8175935\n",
      "17 points at 55.736247 37.476405\n",
      "9 points at 55.929383 37.844581\n",
      "19 points at 55.893155 37.621501\n",
      "\n",
      "Query processing took 6.06398105621 seconds\n",
      "Query: –∑–∞–Ω–æ–≤–æ –∏–ª–∏ –∑–∞–Ω–æ–≤–æ\n",
      "Query: –∑–∞–Ω–æ–≤–æ –∏–ª–∏ –∑–∞–Ω–æ–≤–æ\n",
      "159 points passed preprocessing\n",
      "\n",
      "Query processing took 0.922755002975 seconds\n",
      "Query: –õ–µ—Ç–æ - –Ω–µ –ø–æ–¥–≤–µ–¥–∏ !\n",
      "Query: –ó–Ω–∞–µ—Ç–µ...—É –º–µ–Ω—è –µ—Å—Ç—å —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏—Ü–∞. –£ –∫–æ—Ç–æ—Ä–æ–π —Å—ã–Ω —É–º–µ—Ä –æ—Ç —Ä–∞–∫–∞ –≤ 21 –≥–æ–¥. –í–æ—Ç –æ–Ω–∞ –±—ã–ª–∞ –Ω–∞ –≥—Ä–∞–Ω–∏ —Å—É–º–∞—Å—à–µ—Å—Ç–≤–∏—è. –û–Ω–∞ –≥–æ–¥ –±—ã–ª–∞ –∫–ª–∏–µ–Ω—Ç–æ–º –ø—Å–∏—Ö–∏–∞—Ç—Ä–∞.\n",
      "Query: –∫–∞–∫ –∏–º—è –∫–∞–∫–æ–≥–æ-—Ç–æ —ç–ª—å—Ñ–∞ –±–ª—è—Ç—å –∏–∑ –°—Ä–µ–¥–∏–∑–µ–º—å—è –∏–ª–∏ —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ\n",
      "Query: –ê –≤–æ—Ç –∏ –º—ã- —Ç–≤–æ—Ä—á–µ—Å–∫–∞—è –º–∞—Å—Ç–µ—Ä—Å–∫–∞—è \"–ì–ª–∞–∑—É—Ä—å\" @tmglazur \n",
      "–í—Å–µ–º –∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!\n",
      "–í–∞—Å –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤—É—é—Ç –¥–≤–µ –ê–Ω–Ω—ã.‚Ä¶ https://t.co/RCAmyTfv9m\n",
      "Query: (–≤—Å–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—â–µ–µ –≤ —ç—Ç–æ–º —Ñ–∏–ª—å–º–µ)\n",
      "Query: @Atl_Aztecs –£ –ò–ª—é—Ö–∏ –§–æ–≥–≥–∞.\n",
      "Query: —è —Å–Ω–æ–≤–∞ –≤–∫–ª—é—á–∏–ª–∞ –∫–ª–∏–ø —Å–µ–≤–µ–Ω—Ç–∏–Ω –∏–±–æ —É–¥–∂–∏ :D\n",
      "Query: –Ø –∞—Ö—É–∏—Ç–µ–ª—å–Ω–∞—è, –∞ –Ω–µ –æ—Ö—É–∏—Ç–µ–ª—å–Ω–∞—è. –ü–æ—Ç–æ–º—É —á—Ç–æ –Ω–µ —Ç–∞–∫–∞—è –∫–∞–∫ –≤—Å–µ –æ—Ö—É–∏—Ç–µ–ª—å–Ω—ã–µüòÇ\n",
      "Query: –•–æ—á—É –Ω–∞ –∫—Ä—ã—à—É –≤—ã—Å–æ–∫–æ–≥–æ –∑–¥–∞–Ω–∏—è ,–ø—Ä–æ—Å—Ç–æ —Å–∏–¥–µ—Ç—å –∏ –ª—é–±–æ–≤–∞—Ç—å—Å—è –≤–µ—á–µ—Ä–Ω–µ–π –∫—Ä–∞—Å–æ—Ç–æ–π –ú–æ—Å–∫–≤—ã ‚ú®\n",
      "Query: –≥—Å–ø–¥\n",
      "—Å–µ–π—á–∞—Å –µ—â–µ —Ä–∞–∑ 10 –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ–ª–∞\n",
      "Query: –ó–∞–∫–∞—á–∞–ª –≤ Pocket Book –æ–∫–æ–ª–æ 800 –∫–Ω–∏–≥, –∫–æ–≥–¥–∞ –∏—Ö —Ç–µ–ø–µ—Ä—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ö–∑:))\n",
      "Query: –ü–æ 22 —É –ò–∫–∞—Ä–¥–∏ –∏ –¢–æ–Ω–∏. –í –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ –ª–∞–≤—Ä—ã –ª—É—á—à–µ–≥–æ –≤ —Å–µ—Ä–∏–∏ –ê —Ä–∞–∑–¥–µ–ª—è–ª–∏ –¥–≤–∞ —á–µ–ª–æ–≤–µ–∫–∞(–ü—Ä–æ—Ç—Ç–∏ –∏ –°–∏–Ω—å–æ—Ä–∏) —É–∂–µ –≤ –¥–∞–ª–µ–∫–æ–º 1995 –≥–æ–¥—É!\n",
      "–ü–æ 24!\n",
      "Query: @ananaskalee –∫–∞–∫ –±—Ä–æ—Å–∏—Ç—å –ø–∏—Ç—å\n",
      "Query: @ananaskalee –∫–∞–∫ –±—Ä–æ—Å–∏—Ç—å –ø–∏—Ç—å\n",
      "350 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 7 points among 350\n",
      "7 points at 55.759879 37.632386\n",
      "\n",
      "Query processing took 1.39730215073 seconds\n",
      "Query: –í - –≤–∫—É—Å–µ–Ω—å–∫–æ *0*\n",
      "Query: –°–æ–ª–æ—á –µ–ø—Ç–∞ —Ç–∞—â–∏ #ti5ru\n",
      "Query: –°–æ–ª–æ—á –µ–ø—Ç–∞ —Ç–∞—â–∏ #ti5ru\n",
      "466 points passed preprocessing\n",
      "\n",
      "Query processing took 1.89609098434 seconds\n",
      "Query: –ª–∞–¥–Ω–æ\n",
      "Query: –ª–∞–¥–Ω–æ\n",
      "2445 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 8 spatial clusters with a total of 176 points among 2445\n",
      "8 points at 55.9809335 37.1529465\n",
      "29 points at 55.809453 37.49659\n",
      "29 points at 55.328904 37.809759\n",
      "35 points at 55.599457 37.669212\n",
      "7 points at 55.821945 37.972554\n",
      "32 points at 55.891139 37.7203285\n",
      "23 points at 55.632484 37.522014\n",
      "13 points at 55.942544 37.861787\n",
      "--------------------------------------------------------------\n",
      "Found 2 temporal clusters with a total of 16 points among 2445\n",
      "7 points at 5 . 3 -- 18 : 43 : 24\n",
      "9 points at 4 . 1 -- 13 : 48 : 29\n",
      "\n",
      "Query processing took 14.5523400307 seconds\n",
      "Query: –°–ø–∞—Å–∏–±–æ –°–∞—à—É–ª—å–∫–µ (sashab00 ) –∑–∞ —Ç–∞–∫–∏–µ —á—É–¥–µ—Å–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ. –≠—Ç–æ –±—ã–ª–∏ —à–∏–∫–∞—Ä–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –≤ –æ—Ç–ª–∏—á–Ω–æ–π‚Ä¶ https://t.co/BXkBpEjGeW\n",
      "Query: –°–ø–∞—Å–∏–±–æ –°–∞—à—É–ª—å–∫–µ (sashab00 ) –∑–∞ —Ç–∞–∫–∏–µ —á—É–¥–µ—Å–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ. –≠—Ç–æ –±—ã–ª–∏ —à–∏–∫–∞—Ä–Ω—ã–µ –≤—ã—Ö–æ–¥–Ω—ã–µ –≤ –æ—Ç–ª–∏—á–Ω–æ–π‚Ä¶ https://t.co/BXkBpEjGeW\n",
      "379 points passed preprocessing\n",
      "--------------------------------------------------------------\n",
      "Found 1 spatial clusters with a total of 11 points among 379\n",
      "11 points at 55.7526575 37.613065\n",
      "Caught exception during clusterization\n",
      "Query: @MirFOTOK –ù–∞ 3—Ö –∫–æ–ª–µ–Ω–æ–º –∏ –º–µ–¥–≤–µ–¥–∏ –∫–∞—Ç–∞—é—Ç—Å—è\n",
      "Query: –û–Ω–∏ –≤–µ—á–Ω–æ —Ç–∞–º —Å–æ–±–∞—á–∞—Ç—Å—è! –ó–∞—á–µ–º –≤–æ–æ–±—â–µ –ø—Ä–∏–¥—É–º—ã–≤–∞—é—Ç –ø—Ä–æ–≥—Ä–∞–º–º—ã-–ø–µ—Ä–µ–¥–∞—á–∏ –≥–¥–µ –≤—Å–µ –¥—Ä—É–≥ –¥—Ä—É–≥–∞ –æ–±—Å–∏—Ä–∞—é—Ç! –ò –∫–∞–∫ —Ç–∞–º —Ç–æ–ª—å–∫–æ –¥–æ –¥—Ä–∞–∫–∏ –Ω–µ –¥–æ—Ö–æ–¥–∏—Ç!?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sapi = tweepy.streaming.Stream(auths[2], CustomStreamListener())\n",
    "sapi.filter(locations=[left, down, right, up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = ['123123', '123', '23443']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(t) for t in u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
